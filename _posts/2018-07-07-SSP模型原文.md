---
layout:     post
title:      More Effective Distributed ML via a Stale Synchronous Parallel Parameter Server
subtitle:   使用延迟同步并行参数服务器的更有效的分布式机器学习
date:       2018-08-06
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - paper
    - 翻译
---

## Abstract

&emsp;&emsp;我们为分布式ML提出了一个参数服务器系统，遵循延迟同步并行（SSP）的计算模型，最大限度地提高从节点在运行ML算法的时间，同时仍然提供正确的保证。参数服务器提供了一个易于使用的共享接口，用于读写以及访问ML模型的值（参数和变量），SSP模型允许分布式从节点从本地缓存读取这些值的旧的、延迟的版本，而不是等待从中心存储中获取。这大大增加了从节点花在计算上的比例，而不是等待。此外，SSP模型通过限制延迟的最大值来确保ML算法的正确性。我们提供了SSP模型的正确性证明，以及根据经验结果表明，在几个不同的ML问题上SSP模型相比于完全同步和异步方案实现了更快的算法收敛。
