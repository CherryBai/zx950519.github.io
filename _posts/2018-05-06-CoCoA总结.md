---
layout:     post
title:      CoCoA论文+代码实操
subtitle:   System-Aware Optimization for Machine Learning at Scale
date:       2018-05-06
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - CoCoA
    - Paper
    - Code
---

## 题目——System-Aware Optimization for Machine Learning at Scale
链接：https://escholarship.org/uc/item/20n1k4q8  
作者：Virginia Smith  
时间：August 9, 2017  

## 论文

#### Abstract
&emsp;&emsp;New computing systems have emerged in response to the increasing size and complexity
of modern datasets. For best performance, machine learning methods must be designed to
closely align with the underlying properties of these systems.
In this thesis, we illustrate the impact of system-aware machine learning through the
lens of optimization, a crucial component in formulating and solving most machine learning
problems. Classically, the performance of an optimization method is measured in terms of
accuracy (i.e., does it realize the correct machine learning model? ) and convergence rate
(after how many iterations? ). In modern computing regimes, however, it becomes critical to
additionally consider a number of systems-related aspects for best overall performance. These
aspects can range from low-level details, such as data structures or machine specifications,
to higher-level concepts, such as the tradeoff between communication and computation.
We propose a general optimization framework for machine learning, CoCoA, that gives
careful consideration to systems parameters, often incorporating them directly into the
method and theory. We illustrate the impact of CoCoA in two popular distributed regimes:
the traditional cluster-computing environment, and the increasingly common setting of ondevice
(federated) learning. Our results indicate that by marrying systems-level parameters
and optimization techniques, we can achieve orders-of-magnitude speedups for solving modern
machine learning problems at scale. We corroborate these empirical results by providing
theoretical guarantees that expose systems parameters to give further insight into empirical
performance.  

&emsp;&emsp;由于现代数据集不断增加的大小以及复杂性，新的计算系统应运而生。为了最佳性能，机器学习必须被设计地与系统底层
属性紧密相连。在本文中，我们通过镜像优化(一个解释和解决大多数机器学习问题的重要组件)阐明了系统感知机器学习的影响。具有
典型意义的是，一种优化方法的性能是由其精度衡量(例如，它是否实现了正确的机器学习模型)以及收敛率(经历的多少次迭代)衡量的。
然而在现代计算机体系中，为了整体最佳性能，选择一系列系统相关的方面就变得至关重要。这些方面可以从低级细节，例如数据
结构或机器规格，变化到高级层次，例如通信和计算的平衡。我们提出了一种对机器学习优化一般性优化框架，CoCoA，它仔细考虑了系统
参数，通常将它们直接纳入方法和理论中。我们将在两种流行的分布式系统中阐明CoCoA的影响：传统的集群环境以及越来越流行的ondevice
学习(Google的Federated Learning)。我们的结果表明，通过结合系统级参数和机器进行优化，我们可以实现数量级的加速来解决现代
机器学习的规模问题。我们证实这些经验结果提供理论保证并暴露系统参数，以进一步观察经验性能。

#### Introduction
&emsp;&emsp;面对大规模学习应用的挑战，分布式计算体系结构在现代机器学习中应运而生。分布式体系结构通过提高计算能力和存储容量
来提高可扩展性。保障可扩展性的关键挑战是为分布式机器开发通信和信息协同的高效方法(算法)，并考虑到机器学习算法的特殊需求。  
&emsp;&emsp;在大多数分布式系统中，不同机器间的数据通信较主存中读数据和执行本地计算相比具有更大的开销。此外，计算和通信间的
最佳平衡根据被处理数据变化而变化(往往很大)，使用系统，目标即被优化。因此，分布式方法必须适应灵活的通信计算概况，同时仍然提供
收敛的保证。  
&emsp;&emsp;尽管已经提出了许多分布式优化算法，但小批量最优化方法已经成为了解决这一通信计算交换的最流行例子之一。小批量方法
通过将经典的随机方法推广到在一个时间点处理多个数据点，这有助于通过使每轮通信能够进行更多的分布式计算来缓解通信瓶颈。然而，我
们的需求是减小通信，可以采用增大"小批量方法"的大小，然而这些方法的理论收敛率却随着"小批量"的增大而减小，恢复到了经典(批)梯度
法的速率。实证结果证实了这些理论速率，并且在实践中，小批量方法具有有限的灵活性并适应通信计算折衷，以最大程度地并行执行。此外，
由于小批量方法通常是从特定的单个机器求解器派生的，所以这些方法及其相关的分析通常适合于特定的问题实例，并且当在其受限的问题范围
设置之外应用时，在理论上和实践上也能容忍。  
&emsp;&emsp;在本文中，我提出了一个框架——CoCoA，可以解决这两个基本的限制。首先我们允许任意的本地求解器在每台机器上并行使用。这
允许我们的框架可以直接应用最先进的方法，在分布式设定中使用特定的单机求解器。第二，在我们的框架中，不同机器间的数据共享采用了一种
高度灵活的通信方案。这使得通信量能够容易地适应当前的问题和系统，特别是允许在分布式环境中显著减少通信的情况。  
&emsp;&emsp;在我们的框架中提供这些特征的一个关键步骤是首先为每个机器定义有意义的子问题并行求解，然后以有效的方式组合子问题并更新
解。我们的方法和收敛结果依赖于数据的分布（例如，通过特征或通过训练点），以及我们是否解决了原始或对偶中的问题，某些机器学习目标可以
在分布式设定中更容易地分解成子问题。特别是，我们将常见的机器学习目标归类为几种情况，并使用二元性来帮助分解这些目标。正如我们所展示
的，以这种方式使用原始对偶信息不仅允许高效的方法(例如，与最新的分布式方法相比达到50X倍加速)。但也允许强原始对偶收敛保证和实际利益，
例如计算作为准确性证书和停止标准的对偶间隙。  
#### 1.1 Contributions
&emsp;&emsp;总体框架。我们开发了一个通信有效的原始对偶框架，适用于广泛的一类凸优化问题。值得注意的是，与先前的[38, 51, 100]和[53]
的工作相比，我们的概括的、内聚的框架：  
（1）具体地说，结合L1正则化和其他非强凸正则化的困难情况  
（2）允许通过特征或训练点分发数据的灵活性  
（3）可以在原始或对偶公式上运行，这表明我们有重要的理论和实践意义。  
  
&emsp;&emsp;灵活的沟通和本地解决方案。所提出的框架的两个主要优点是其通信效率和内部采用现成的单机求解器的能力。在现实世界的系统中，
通信与计算的开销可以有很大的不同，因此根据手动设置允许灵活的通信量是有利的。我们的框架正好提供了这样的控制。此外，我们允许在每个机
器上使用任意的求解器，这允许重用现有代码和来自多核或其他优化的好处。  

&emsp;&emsp;原始对偶率。我们为我们的框架推导出收敛速度，利用一种新的方法在分析原始双率非强凸正则化。所提出的技术是相对于简单的平滑
技术（例如，[67, 84）和[110 ]中通过对目标添加小L2项来实施强凸性的显著改进。我们的结果包括原始对偶率和证书的线性正则化损失最小化的
一般类，我们展示了早期的工作如何可以作为一个特殊的情况下导出更一般的方法。  

&emsp;&emsp;实验比较。相比于最先进的大规模机器学习方法，所提出的框架产生数量级的速度提升（高达50×或更大）。我们在真实世界的分布式
数据集展示了这些性能增益并做了广泛的实验。我们还探索框架本身的属性，包括在原始或对偶框架中运行框架的效果。比较算法都是在Apache Spark
中实现的，并且运行在Amazon EC2集群上。我们的代码是开源的，在github.com/gingsmith/proxcocoa中公开可用。  

&emsp;&emsp;联合学习。最后，我们探讨了在各种分布式计算环境中的框架，包括联合学习的新领域，其目的是在低功率设备的大型网络上进行优化。
我们提出了一个CoCoA扩展，Mocha，这是理想的适合处理独特的系统和统计挑战的联合环境。通过对真实世界的联合数据集仿真，我们证明了该方法的
优越的统计性能和经验加速，并提供了一个仔细的理论分析，探讨了系统的挑战，如干扰和容错对保证收敛的影响。

#### 1.2 Related Work
&emsp;&emsp;单机坐标求解器。对于强凸正则化器，现有的经验损失最小化的状态是双坐标（SDCA）〔85〕及其加速变型的随机化坐标上升〔84〕。
与原始随机梯度下降（SGD）方法相比，SDCA族通常是优选的，因为它没有学习速率参数，并且具有更快（几何）收敛保证。有趣的是，类似的趋势
最近在坐标求解器中已经观察到文献使用lasso，但具有原始和双重反转的作用。对于这些问题，原始的坐标下降方法已经成为最新的技术。  

&emsp;&emsp;L1正则化问题的坐标下降可以解释为以光滑部分为目标的二次近似的迭代最小化（如一维牛顿步骤），其次是L1部分产生的收缩步长。
在单坐标更新情况下，这是GLMNET（28106）的核心，并广泛应用于基于L1正则化目标的原始公式的求解器[12, 27, 82，91, 105]。当一次改变一
个以上坐标时，再次采用光滑部分上的二次上界，这导致了Logistic回归的特殊情况下的GLMNET中的双回路方法。这一思想对于分布式设置至关重要
，当主动坐标集与本地机器上的坐标集一致时，这些单机方法非常类似于这里提出的分布式框架。  

&emsp;&emsp;并行方法。对于一般的正则化损失最小化问题，建立了基于随机次梯度下降（SGD）的方法。SGD的几种变型已经被提出用于并行计算，
其中许多基于了异步通信的思想[23, 68]。尽管它们在共享内存系统上具有简单性和竞争性能，但这种方法在分布式环境中的缺点是所需通信量等于
本地读取的数据量，因为每轮每台机器访问一个数据点。这些变型在实践中与在本工作中考虑的更多通信高效的方法不具有竞争性，这允许每个通信
回合能进行更多的本地更新。  

&emsp;&emsp;对于L1正则化目标的具体情况，在[17](shotgun?啥意思，散弹枪？)中提出了并行坐标下降（有和不使用小批次），并在[12]中推广;
它是并行环境中性能最好的求解器之一。当内部求解器是子问题上的单坐标更新时，我们的框架减少成为Shotgun(霰弹枪?)。然而，Shotgun没有被
我们的收敛理论所覆盖，因为它使用了一个潜在不安全的上界β，而不是α，这不能保证满足我们的收敛条件（2.11）。在第4章中，我们用Shotgun
来强调在分布式环境中运行这种高通信(high-communication)方法的不利影响。  

&emsp;&emsp;一次性通信方案。在另一个极端，有一些方法只使用一轮通信[参见57, 62, 109，112, 32 ]。这些方法需要对数据分区做额外的假设，
如果数据按"as is"分布，则通常在实践中不满足，即，如果我们事先没有机会以特定的方式分发数据。此外，如果我们忽略了一台计算机之外的所有
数据，如[86]中所示，有些情况(some)不能保证收敛率超过可以达到的水平。在[8]和[7]中给出了对于给定近似质量所必需通信回合的最小数目的附加
相关下界。  

&emsp;&emsp;小批量方法。小批量方法（使用来自几个训练点或每轮特征的更新）更灵活，并且位于并行和一次性通信方案的两个极端之间。然而，
SGD和坐标下降（CD）（例如，[72, 79, 83，90]）的小批量版本由于小批量的粒度增加而遭受它们的收敛速率下降到批处理梯度下降的速率。这是因为
使用了过期的参数向量W进行了小批量更新，与允许CoCoA的即时本地更新的方法相反。  
&emsp;&emsp;小批量方法的另一个缺点是聚合参数更难调整，因为它可以以最小批量大小出现在任何地方。在实践中计算最佳选择往往是未知的或太具有
挑战性。在CoCoA框架中，不需要调整参数，因为聚合参数和子问题参数可以直接使用第2章（定义5）中所讨论的安全界限来设置。  

&emsp;&emsp;批量求解器。ADMM[16]、梯度下降和拟牛顿方法（如L-BFGS）也经常用于分布式环境中，因为它们的通信需求相对较低。然而，它们需要在
每一轮至少有一个完整的（分布式）批处理梯度计算，因此不允许CoCoA提供的通信和计算之间的平衡。在第4章中，我们包括与ADMM、梯度下降和L-BFGS
变型的实验比较，包括L1设置[3]的正方形受限的有限存储器拟牛顿（OWL QN）。
