---
layout:     post
title:      CoCoA论文+代码实操
subtitle:   System-Aware Optimization for Machine Learning at Scale
date:       2018-05-06
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - CoCoA
    - Paper
    - Code
---

## 题目——System-Aware Optimization for Machine Learning at Scale
链接：https://escholarship.org/uc/item/20n1k4q8  
作者：Virginia Smith  
时间：August 9, 2017  

## 论文

#### Abstract
&emsp;&emsp;New computing systems have emerged in response to the increasing size and complexity
of modern datasets. For best performance, machine learning methods must be designed to
closely align with the underlying properties of these systems.
In this thesis, we illustrate the impact of system-aware machine learning through the
lens of optimization, a crucial component in formulating and solving most machine learning
problems. Classically, the performance of an optimization method is measured in terms of
accuracy (i.e., does it realize the correct machine learning model? ) and convergence rate
(after how many iterations? ). In modern computing regimes, however, it becomes critical to
additionally consider a number of systems-related aspects for best overall performance. These
aspects can range from low-level details, such as data structures or machine specifications,
to higher-level concepts, such as the tradeoff between communication and computation.
We propose a general optimization framework for machine learning, CoCoA, that gives
careful consideration to systems parameters, often incorporating them directly into the
method and theory. We illustrate the impact of CoCoA in two popular distributed regimes:
the traditional cluster-computing environment, and the increasingly common setting of ondevice
(federated) learning. Our results indicate that by marrying systems-level parameters
and optimization techniques, we can achieve orders-of-magnitude speedups for solving modern
machine learning problems at scale. We corroborate these empirical results by providing
theoretical guarantees that expose systems parameters to give further insight into empirical
performance.  

&emsp;&emsp;由于现代数据集不断增加的大小以及复杂性，新的计算系统应运而生。为了最佳性能，机器学习必须被设计地与系统底层
属性紧密相连。在本文中，我们通过镜像优化(一个解释和解决大多数机器学习问题的重要组件)阐明了系统感知机器学习的影响。具有
典型意义的是，一种优化方法的性能是由其精度衡量(例如，它是否实现了正确的机器学习模型)以及收敛率(经历的多少次迭代)衡量的。
然而在现代计算机体系中，为了整体最佳性能，选择一系列系统相关的方面就变得至关重要。这些方面可以从低级细节，例如数据
结构或机器规格，变化到高级层次，例如通信和计算的平衡。我们提出了一种对机器学习优化一般性优化框架，CoCoA，它仔细考虑了系统
参数，通常将它们直接纳入方法和理论中。我们将在两种流行的分布式系统中阐明CoCoA的影响：传统的集群环境以及越来越流行的ondevice
学习(Google的Federated Learning)。我们的结果表明，通过结合系统级参数和机器进行优化，我们可以实现数量级的加速来解决现代
机器学习的规模问题。我们证实这些经验结果提供理论保证并暴露系统参数，以进一步观察经验性能。

#### Introduction
&emsp;&emsp;面对大规模学习应用的挑战，分布式计算体系结构在现代机器学习中应运而生。分布式体系结构通过提高计算能力和存储容量
来提高可扩展性。保障可扩展性的关键挑战是为分布式机器开发通信和信息协同的高效方法(算法)，并考虑到机器学习算法的特殊需求。  
&emsp;&emsp;在大多数分布式系统中，不同机器间的数据通信较主存中读数据和执行本地计算相比具有更大的开销。此外，计算和通信间的
最佳平衡根据被处理数据变化而变化(往往很大)，使用系统，目标即被优化。因此，分布式方法必须适应灵活的通信计算概况，同时仍然提供
收敛的保证。  
&emsp;&emsp;尽管已经提出了许多分布式优化算法，但小批量最优化方法已经成为了解决这一通信计算交换的最流行例子之一。小批量方法
通过将经典的随机方法推广到在一个时间点处理多个数据点，这有助于通过使每轮通信能够进行更多的分布式计算来缓解通信瓶颈。然而，我
们的需求是减小通信，可以采用增大"小批量方法"的大小，然而这些方法的理论收敛率却随着"小批量"的增大而减小，恢复到了经典(批)梯度
法的速率。实证结果证实了这些理论速率，并且在实践中，小批量方法具有有限的灵活性并适应通信计算折衷，以最大程度地并行执行。此外，
由于小批量方法通常是从特定的单个机器求解器派生的，所以这些方法及其相关的分析通常适合于特定的问题实例，并且当在其受限的问题范围
设置之外应用时，在理论上和实践上也能容忍。  
&emsp;&emsp;在本文中，我提出了一个框架——CoCoA，可以解决这两个基本的限制。首先我们允许任意的本地求解器在每台机器上并行使用。这
允许我们的框架可以直接应用最先进的方法，在分布式设定中使用特定的单机求解器。第二，在我们的框架中，不同机器间的数据共享采用了一种
高度灵活的通信方案。这使得通信量能够容易地适应当前的问题和系统，特别是允许在分布式环境中显著减少通信的情况。  
&emsp;&emsp;在我们的框架中提供这些特征的一个关键步骤是首先为每个机器定义有意义的子问题并行求解，然后以有效的方式组合子问题并更新
解。我们的方法和收敛结果依赖于数据的分布（例如，通过特征或通过训练点），以及我们是否解决了原始或对偶中的问题，某些机器学习目标可以
在分布式设定中更容易地分解成子问题。特别是，我们将常见的机器学习目标归类为几种情况，并使用二元性来帮助分解这些目标。正如我们所展示
的，以这种方式使用原始对偶信息不仅允许高效的方法(例如，与最新的分布式方法相比达到50X倍加速)。但也允许强原始对偶收敛保证和实际利益，
例如计算作为准确性证书和停止标准的对偶间隙。
