---
layout:     post
title:      CoCoA论文+代码实操
subtitle:   System-Aware Optimization for Machine Learning at Scale
date:       2018-05-06
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - CoCoA
    - Paper
    - Code
---

## 题目——System-Aware Optimization for Machine Learning at Scale
链接：https://escholarship.org/uc/item/20n1k4q8  
作者：Virginia Smith  
时间：August 9, 2017  

## 论文

#### Abstract
&emsp;&emsp;New computing systems have emerged in response to the increasing size and complexity
of modern datasets. For best performance, machine learning methods must be designed to
closely align with the underlying properties of these systems.
In this thesis, we illustrate the impact of system-aware machine learning through the
lens of optimization, a crucial component in formulating and solving most machine learning
problems. Classically, the performance of an optimization method is measured in terms of
accuracy (i.e., does it realize the correct machine learning model? ) and convergence rate
(after how many iterations? ). In modern computing regimes, however, it becomes critical to
additionally consider a number of systems-related aspects for best overall performance. These
aspects can range from low-level details, such as data structures or machine specifications,
to higher-level concepts, such as the tradeoff between communication and computation.
We propose a general optimization framework for machine learning, CoCoA, that gives
careful consideration to systems parameters, often incorporating them directly into the
method and theory. We illustrate the impact of CoCoA in two popular distributed regimes:
the traditional cluster-computing environment, and the increasingly common setting of ondevice
(federated) learning. Our results indicate that by marrying systems-level parameters
and optimization techniques, we can achieve orders-of-magnitude speedups for solving modern
machine learning problems at scale. We corroborate these empirical results by providing
theoretical guarantees that expose systems parameters to give further insight into empirical
performance.  

&emsp;&emsp;由于现代数据集不断增加的大小以及复杂性，新的计算系统应运而生。为了最佳性能，机器学习必须被设计地与系统底层
属性紧密相连。在本文中，我们通过镜像优化(一个解释和解决大多数机器学习问题的重要组件)阐明了系统感知机器学习的影响。具有
典型意义的是，一种优化方法的性能是由其精度衡量(例如，它是否实现了正确的机器学习模型)以及收敛率(经历的多少次迭代)衡量的。
然而在现代计算机体系中，为了整体最佳性能，选择一系列系统相关的方面就变得至关重要。这些方面可以从低级细节，例如数据
结构或机器规格，变化到高级层次，例如通信和计算的平衡。我们提出了一种对机器学习优化一般性优化框架，CoCoA，它仔细考虑了系统
参数，通常将它们直接纳入方法和理论中。我们将在两种流行的分布式系统中阐明CoCoA的影响：传统的集群环境以及越来越流行的ondevice
学习(Google的Federated Learning)。我们的结果表明，通过结合系统级参数和机器进行优化，我们可以实现数量级的加速来解决现代
机器学习的规模问题。我们证实这些经验结果提供理论保证并暴露系统参数，以进一步观察经验性能。

#### Introduction
&emsp;&emsp;面对大规模学习应用的挑战，分布式计算体系结构在现代机器学习中应运而生。分布式体系结构通过提高计算能力和存储容量
来提高可扩展性。保障可扩展性的关键挑战是为分布式机器开发通信和信息协同的高效方法(算法)，并考虑到机器学习算法的特殊需求。  
&emsp;&emsp;在大多数分布式系统中，不同机器间的数据通信较主存中读数据和执行本地计算相比具有更大的开销。此外，计算和通信间的
最佳平衡根据被处理数据变化而变化(往往很大)，使用系统，目标即被优化。因此，分布式方法必须适应灵活的通信计算概况，同时仍然提供
收敛的保证。  
&emsp;&emsp;尽管已经提出了许多分布式优化算法，但小批量最优化方法已经成为了解决这一通信计算交换的最流行例子之一。小批量方法
通过将经典的随机方法推广到在一个时间点处理多个数据点，这有助于通过使每轮通信能够进行更多的分布式计算来缓解通信瓶颈。然而，我
们的需求是减小通信，可以采用增大"小批量方法"的大小，然而这些方法的理论收敛率却随着"小批量"的增大而减小，恢复到了经典(批)梯度
法的速率。实证结果证实了这些理论速率，并且在实践中，小批量方法具有有限的灵活性并适应通信计算折衷，以最大程度地并行执行。此外，
由于小批量方法通常是从特定的单个机器求解器派生的，所以这些方法及其相关的分析通常适合于特定的问题实例，并且当在其受限的问题范围
设置之外应用时，在理论上和实践上也能容忍。  
&emsp;&emsp;在本文中，我提出了一个框架——CoCoA，可以解决这两个基本的限制。首先我们允许任意的本地求解器在每台机器上并行使用。这
允许我们的框架可以直接应用最先进的方法，在分布式设定中使用特定的单机求解器。第二，在我们的框架中，不同机器间的数据共享采用了一种
高度灵活的通信方案。这使得通信量能够容易地适应当前的问题和系统，特别是允许在分布式环境中显著减少通信的情况。  
&emsp;&emsp;在我们的框架中提供这些特征的一个关键步骤是首先为每个机器定义有意义的子问题并行求解，然后以有效的方式组合子问题并更新
解。我们的方法和收敛结果依赖于数据的分布（例如，通过特征或通过训练点），以及我们是否解决了原始或对偶中的问题，某些机器学习目标可以
在分布式设定中更容易地分解成子问题。特别是，我们将常见的机器学习目标归类为几种情况，并使用二元性来帮助分解这些目标。正如我们所展示
的，以这种方式使用原始对偶信息不仅允许高效的方法(例如，与最新的分布式方法相比达到50X倍加速)。但也允许强原始对偶收敛保证和实际利益，
例如计算作为准确性证书和停止标准的对偶间隙。  
#### 1.1 Contributions
&emsp;&emsp;总体框架。我们开发了一个通信有效的原始对偶框架，适用于广泛的一类凸优化问题。值得注意的是，与先前的[38, 51, 100]和[53]
的工作相比，我们的概括的、内聚的框架：  
（1）具体地说，结合L1正则化和其他非强凸正则化的困难情况  
（2）允许通过特征或训练点分发数据的灵活性  
（3）可以在原始或对偶公式上运行，这表明我们有重要的理论和实践意义。  
  
&emsp;&emsp;灵活的沟通和本地解决方案。所提出的框架的两个主要优点是其通信效率和内部采用现成的单机求解器的能力。在现实世界的系统中，
通信与计算的开销可以有很大的不同，因此根据手动设置允许灵活的通信量是有利的。我们的框架正好提供了这样的控制。此外，我们允许在每个机
器上使用任意的求解器，这允许重用现有代码和来自多核或其他优化的好处。  

&emsp;&emsp;原始对偶率。我们为我们的框架推导出收敛速度，利用一种新的方法在分析原始双率非强凸正则化。所提出的技术是相对于简单的平滑
技术（例如，[67, 84）和[110 ]中通过对目标添加小L2项来实施强凸性的显著改进。我们的结果包括原始对偶率和证书的线性正则化损失最小化的
一般类，我们展示了早期的工作如何可以作为一个特殊的情况下导出更一般的方法。  

&emsp;&emsp;实验比较。相比于最先进的大规模机器学习方法，所提出的框架产生数量级的速度提升（高达50×或更大）。我们在真实世界的分布式
数据集展示了这些性能增益并做了广泛的实验。我们还探索框架本身的属性，包括在原始或对偶框架中运行框架的效果。比较算法都是在Apache Spark
中实现的，并且运行在Amazon EC2集群上。我们的代码是开源的，在github.com/gingsmith/proxcocoa中公开可用。  

&emsp;&emsp;联合学习。最后，我们探讨了在各种分布式计算环境中的框架，包括联合学习的新领域，其目的是在低功率设备的大型网络上进行优化。
我们提出了一个CoCoA扩展，Mocha，这是理想的适合处理独特的系统和统计挑战的联合环境。通过对真实世界的联合数据集仿真，我们证明了该方法的
优越的统计性能和经验加速，并提供了一个仔细的理论分析，探讨了系统的挑战，如干扰和容错对保证收敛的影响。
