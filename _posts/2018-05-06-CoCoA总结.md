---
layout:     post
title:      CoCoA论文+代码实操
subtitle:   System-Aware Optimization for Machine Learning at Scale
date:       2018-05-06
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - CoCoA
    - Paper
    - Code
---

## 题目——System-Aware Optimization for Machine Learning at Scale
链接：https://escholarship.org/uc/item/20n1k4q8  
作者：Virginia Smith  
时间：August 9, 2017  

## 论文

#### Abstract
&emsp;&emsp;New computing systems have emerged in response to the increasing size and complexity
of modern datasets. For best performance, machine learning methods must be designed to
closely align with the underlying properties of these systems.
In this thesis, we illustrate the impact of system-aware machine learning through the
lens of optimization, a crucial component in formulating and solving most machine learning
problems. Classically, the performance of an optimization method is measured in terms of
accuracy (i.e., does it realize the correct machine learning model? ) and convergence rate
(after how many iterations? ). In modern computing regimes, however, it becomes critical to
additionally consider a number of systems-related aspects for best overall performance. These
aspects can range from low-level details, such as data structures or machine specifications,
to higher-level concepts, such as the tradeoff between communication and computation.
We propose a general optimization framework for machine learning, CoCoA, that gives
careful consideration to systems parameters, often incorporating them directly into the
method and theory. We illustrate the impact of CoCoA in two popular distributed regimes:
the traditional cluster-computing environment, and the increasingly common setting of ondevice
(federated) learning. Our results indicate that by marrying systems-level parameters
and optimization techniques, we can achieve orders-of-magnitude speedups for solving modern
machine learning problems at scale. We corroborate these empirical results by providing
theoretical guarantees that expose systems parameters to give further insight into empirical
performance.  

&emsp;&emsp;由于现代数据集不断增加的大小以及复杂性，新的计算系统应运而生。为了最佳性能，机器学习必须被设计地与系统底层
属性紧密相连。在本文中，我们通过镜像优化(一个解释和解决大多数机器学习问题的重要组件)阐明了系统感知机器学习的影响。具有
典型意义的是，一种优化方法的性能是由其精度衡量(例如，它是否实现了正确的机器学习模型)以及收敛率(经历的多少次迭代)衡量的。
然而在现代计算机体系中，为了整体最佳性能，选择一系列系统相关的方面就变得至关重要。这些方面可以从低级细节，例如数据
结构或机器规格，变化到高级层次，例如通信和计算的平衡。我们提出了一种对机器学习优化一般性优化框架，CoCoA，它仔细考虑了系统
参数，通常将它们直接纳入方法和理论中。我们将在两种流行的分布式系统中阐明CoCoA的影响：传统的集群环境以及越来越流行的ondevice
学习(Google的Federated Learning)。我们的结果表明，通过结合系统级参数和机器进行优化，我们可以实现数量级的加速来解决现代
机器学习的规模问题。我们证实这些经验结果提供理论保证并暴露系统参数，以进一步观察经验性能。

#### Introduction
&emsp;&emsp;面对大规模学习应用的挑战，分布式计算体系结构在现代机器学习中应运而生。分布式体系结构通过提高计算能力和存储容量
来提高可扩展性。保障可扩展性的关键挑战是为分布式机器开发通信和信息协同的高效方法(算法)，并考虑到机器学习算法的特殊需求。  
&emsp;&emsp;在大多数分布式系统中，不同机器间的数据通信较主存中读数据和执行本地计算相比具有更大的开销。此外，计算和通信间的
最佳平衡根据被处理数据变化而变化(往往很大)，使用系统，目标即被优化。因此，分布式方法必须适应灵活的通信计算概况，同时仍然提供
收敛的保证。  
&emsp;&emsp;尽管已经提出了许多分布式优化算法，但小批量最优化方法已经成为了解决这一通信计算交换的最流行例子之一。小批量方法
通过将经典的随机方法推广到在一个时间点处理多个数据点，这有助于通过使每轮通信能够进行更多的分布式计算来缓解通信瓶颈。然而，我
们的需求是减小通信，可以采用增大"小批量方法"的大小，然而这些方法的理论收敛率却随着"小批量"的增大而减小，恢复到了经典(批)梯度
法的速率。实证结果证实了这些理论速率，并且在实践中，小批量方法具有有限的灵活性并适应通信计算折衷，以最大程度地并行执行。此外，
由于小批量方法通常是从特定的单个机器求解器派生的，所以这些方法及其相关的分析通常适合于特定的问题实例，并且当在其受限的问题范围
设置之外应用时，在理论上和实践上也能容忍。  
&emsp;&emsp;在本文中，我提出了一个框架——CoCoA，可以解决这两个基本的限制。首先我们允许任意的本地求解器在每台机器上并行使用。这
允许我们的框架可以直接应用最先进的方法，在分布式设定中使用特定的单机求解器。第二，在我们的框架中，不同机器间的数据共享采用了一种
高度灵活的通信方案。这使得通信量能够容易地适应当前的问题和系统，特别是允许在分布式环境中显著减少通信的情况。  
&emsp;&emsp;在我们的框架中提供这些特征的一个关键步骤是首先为每个机器定义有意义的子问题并行求解，然后以有效的方式组合子问题并更新
解。我们的方法和收敛结果依赖于数据的分布（例如，通过特征或通过训练点），以及我们是否解决了原始或对偶中的问题，某些机器学习目标可以
在分布式设定中更容易地分解成子问题。特别是，我们将常见的机器学习目标归类为几种情况，并使用二元性来帮助分解这些目标。正如我们所展示
的，以这种方式使用原始对偶信息不仅允许高效的方法(例如，与最新的分布式方法相比达到50X倍加速)。但也允许强原始对偶收敛保证和实际利益，
例如计算作为准确性证书和停止标准的对偶间隙。  
#### 1.1 Contributions
&emsp;&emsp;总体框架。我们开发了一个通信有效的原始对偶框架，适用于广泛的一类凸优化问题。值得注意的是，与先前的[38, 51, 100]和[53]
的工作相比，我们的概括的、内聚的框架：  
（1）具体地说，结合L1正则化和其他非强凸正则化的困难情况  
（2）允许通过特征或训练点分发数据的灵活性  
（3）可以在原始或对偶公式上运行，这表明我们有重要的理论和实践意义。  
  
&emsp;&emsp;灵活的沟通和本地解决方案。所提出的框架的两个主要优点是其通信效率和内部采用现成的单机求解器的能力。在现实世界的系统中，
通信与计算的开销可以有很大的不同，因此根据手动设置允许灵活的通信量是有利的。我们的框架正好提供了这样的控制。此外，我们允许在每个机
器上使用任意的求解器，这允许重用现有代码和来自多核或其他优化的好处。  

&emsp;&emsp;原始对偶率。我们为我们的框架推导出收敛速度，利用一种新的方法在分析原始双率非强凸正则化。所提出的技术是相对于简单的平滑
技术（例如，[67, 84）和[110 ]中通过对目标添加小L2项来实施强凸性的显著改进。我们的结果包括原始对偶率和证书的线性正则化损失最小化的
一般类，我们展示了早期的工作如何可以作为一个特殊的情况下导出更一般的方法。  

&emsp;&emsp;实验比较。相比于最先进的大规模机器学习方法，所提出的框架产生数量级的速度提升（高达50×或更大）。我们在真实世界的分布式
数据集展示了这些性能增益并做了广泛的实验。我们还探索框架本身的属性，包括在原始或对偶框架中运行框架的效果。比较算法都是在Apache Spark
中实现的，并且运行在Amazon EC2集群上。我们的代码是开源的，在github.com/gingsmith/proxcocoa中公开可用。  

&emsp;&emsp;联合学习。最后，我们探讨了在各种分布式计算环境中的框架，包括联合学习的新领域，其目的是在低功率设备的大型网络上进行优化。
我们提出了一个CoCoA扩展，Mocha，这是理想的适合处理独特的系统和统计挑战的联合环境。通过对真实世界的联合数据集仿真，我们证明了该方法的
优越的统计性能和经验加速，并提供了一个仔细的理论分析，探讨了系统的挑战，如干扰和容错对保证收敛的影响。

#### 1.2 Related Work
&emsp;&emsp;单机坐标求解器。对于强凸正则化器，现有的经验损失最小化的状态是双坐标（SDCA）〔85〕及其加速变型的随机化坐标上升〔84〕。
与原始随机梯度下降（SGD）方法相比，SDCA族通常是优选的，因为它没有学习速率参数，并且具有更快（几何）收敛保证。有趣的是，类似的趋势
最近在坐标求解器中已经观察到文献使用lasso，但具有原始和双重反转的作用。对于这些问题，原始的坐标下降方法已经成为最新的技术。  

&emsp;&emsp;L1正则化问题的坐标下降可以解释为以光滑部分为目标的二次近似的迭代最小化（如一维牛顿步骤），其次是L1部分产生的收缩步长。
在单坐标更新情况下，这是GLMNET（28106）的核心，并广泛应用于基于L1正则化目标的原始公式的求解器[12, 27, 82，91, 105]。当一次改变一
个以上坐标时，再次采用光滑部分上的二次上界，这导致了Logistic回归的特殊情况下的GLMNET中的双回路方法。这一思想对于分布式设置至关重要
，当主动坐标集与本地机器上的坐标集一致时，这些单机方法非常类似于这里提出的分布式框架。  

&emsp;&emsp;并行方法。对于一般的正则化损失最小化问题，建立了基于随机次梯度下降（SGD）的方法。SGD的几种变型已经被提出用于并行计算，
其中许多基于了异步通信的思想[23, 68]。尽管它们在共享内存系统上具有简单性和竞争性能，但这种方法在分布式环境中的缺点是所需通信量等于
本地读取的数据量，因为每轮每台机器访问一个数据点。这些变型在实践中与在本工作中考虑的更多通信高效的方法不具有竞争性，这允许每个通信
回合能进行更多的本地更新。  

&emsp;&emsp;对于L1正则化目标的具体情况，在[17](shotgun?啥意思，散弹枪？)中提出了并行坐标下降（有和不使用小批次），并在[12]中推广;
它是并行环境中性能最好的求解器之一。当内部求解器是子问题上的单坐标更新时，我们的框架减少成为Shotgun(霰弹枪?)。然而，Shotgun没有被
我们的收敛理论所覆盖，因为它使用了一个潜在不安全的上界β，而不是α，这不能保证满足我们的收敛条件（2.11）。在第4章中，我们用Shotgun
来强调在分布式环境中运行这种高通信(high-communication)方法的不利影响。  

&emsp;&emsp;一次性通信方案。在另一个极端，有一些方法只使用一轮通信[参见57, 62, 109，112, 32 ]。这些方法需要对数据分区做额外的假设，
如果数据按"as is"分布，则通常在实践中不满足，即，如果我们事先没有机会以特定的方式分发数据。此外，如果我们忽略了一台计算机之外的所有
数据，如[86]中所示，有些情况(some)不能保证收敛率超过可以达到的水平。在[8]和[7]中给出了对于给定近似质量所必需通信回合的最小数目的附加
相关下界。  

&emsp;&emsp;小批量方法。小批量方法（使用来自几个训练点或每轮特征的更新）更灵活，并且位于并行和一次性通信方案的两个极端之间。然而，
SGD和坐标下降（CD）（例如，[72, 79, 83，90]）的小批量版本由于小批量的粒度增加而遭受它们的收敛速率下降到批处理梯度下降的速率。这是因为
使用了过期的参数向量W进行了小批量更新，与允许CoCoA的即时本地更新的方法相反。  
&emsp;&emsp;小批量方法的另一个缺点是聚合参数更难调整，因为它可以以最小批量大小出现在任何地方。在实践中计算最佳选择往往是未知的或太具有
挑战性。在CoCoA框架中，不需要调整参数，因为聚合参数和子问题参数可以直接使用第2章（定义5）中所讨论的安全界限来设置。  

&emsp;&emsp;批量求解器。ADMM[16]、梯度下降和拟牛顿方法（如L-BFGS）也经常用于分布式环境中，因为它们的通信需求相对较低。然而，它们需要在
每一轮至少有一个完整的（分布式）批处理梯度计算，因此不允许CoCoA提供的通信和计算之间的平衡。在第4章中，我们包括与ADMM、梯度下降和L-BFGS
变型的实验比较，包括L1设置[3]的正方形受限的有限存储器拟牛顿（OWL QN）。  

&emsp;&emsp;最后，我们注意到，虽然对于COCOA的收敛速度反映了经典的批梯度法在外圆数上的收敛类，但现有的批梯度法却具有较弱的理论，因为它们
不允许局部子问题的一般不精确性。相反，我们的收敛速率直接包含这种近似，而且，对于任意局部求解器来说，比批处理方法开销小得多（在每一轮中，
每一台机器必须精确地通过局部数据完全处理）。这使得CoCoA在分布式环境中更加灵活，因为它可以适应不同的通信成本在实际系统中。我们已经在第4章
中看到，这种灵活性导致在竞争方法上获得显著的性能增益。  

&emsp;&emsp;分布式求解器。利用〔70100, 101, 103〕和〔48〕工作线中的原始对偶结构，CoCoA-v1和CoCoA+框架是第一个允许在分布式环境中使用
每一轮中的弱局部近似质量的任何局部解算器。DISDCA[100]的实际变型，称为DISDCA-P，允许以类似的方式添加到COCOA的更新，但被限制为z坐标下降
（CD）作为局部解算器，并且最初提出没有收敛保证。DISDCA-P、COCO-V1和COCOA+都局限于强凸正则化器，因此不像在这项工作中讨论的CoCoA框架具有
一般性。  

&emsp;&emsp;在L1正则化设置中，与我们的框架相关的方法包括在[56]中的GLMNET的分布式变型。受GLMNET和[105]的启发，[12]和[56]的著作在分布
L1上下文中引入了块对角Hessian上近似的思想。后来的工作[92]专门用于稀疏logistic回归的方法。  

#### 1.3 Organization

&emsp;&emsp;本文的其余部分整理如下。第2章介绍了分布式优化的CoCoA框架。我们首先提供必要的背景，包括标准定义的优化和对偶。使用所提出的原始
对偶结构，然后我们的问题的情况下，我们的框架，我们阐明了一组运行的例子。接下来，我们详细介绍CoCoA，并解释框架如何在分布式环境中以原始或对
偶运行。该方法的两个关键部分是更新方案和子问题的公式化，我们通过探索这些组件的几种解释并将方法与相关工作进行比较来结束本章。  

&emsp;&emsp;所提出的方法适用于机器学习和信号处理中的许多常见问题。第3章详细介绍了可以通过通用COCOA框架实现的几个示例应用程序。对于每一个
应用程序，我们描述了原始对偶设置和算法细节；讨论了我们的框架的收敛性的应用程序，并包括实际问题，如有关本地解算器的信息。  

&emsp;&emsp;第4章和第5章分别提供了我们的经验和理论结果。在第4章中，我们将CoCoA与分布式数据中心设置中的其他最先进的求解器进行了比较。我们的
结果表明，在解决现实世界分布式数据集上常见的机器学习问题的上有着数量级加速。为了补充这些比较，我们探索框架的各种性质，包括原始与对偶式优化的
权衡，通信对框架的影响，以及子问题公式的影响。在第5章中，我们得到了框架的收敛保证，并证明了前面章节中给出的所有其它结果。我们的收敛结果包括
原始对偶率和证书的一般类线性正则化损失最小化。在这些结果中的一个关键贡献是我们能够将局部解算器的性能抽象为每个分布式机器上定义的子问题的近似
解。我们的收敛保证是通过将这个局部收敛与全局解的改进联系起来的。  

&emsp;&emsp;最后，在第6章中，我们将CoCoA扩展到联合学习，这是一个日益常见的场景，其中机器学习模型的训练在分布式设备上进行。这种设置提出了新
的统计挑战，在我们的建模方法、模型的培训，以及新的系统挑战。特别地，诸如非IID数据、散乱器和容错的问题比在典型的数据中心设置中更普遍。为了应
对这些挑战，我们提出Mocha，一种方法（1）利用多任务学习，（2）执行目标的交替最小化，和（3）扩展COCOA以执行联合MTL更新。由此产生的框架具有优
越的统计性能和相对于竞争者的高度灵活的优化方案。我们演示了该方法的实证表现，通过模拟真实世界的联合数据集，并提供仔细的理论分析，探讨联合学习
中普遍存在的挑战对我们的收敛保证的影响。

#### Chapter 2——CoCoA Framework
&emsp;&emsp;在本工作中，我们为形如下面类型的最小化问题开发了一个基本框架： 
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr2v8gijdhj3068027q2r.jpg)  

&emsp;&emsp;对于凸函数l和r。通常第一项是数据上的经验损失，通常取形式：l1(u)+l2(u)+...+li(u)(求和公式展开)，第二项是正则项，例如r(u)=λ||u||。该公式在机器学习和信号处理中十分流行，如支持向量机、线性和Logistics回归、Lasso和稀疏logistic回归以及许多其他方法。  

#### 2.1 Notation
&emsp;&emsp;本论文将使用以下标准定义:  

定义1((L-Lipschitz连续)，参考：https://zh.wikipedia.org/wiki/%E5%88%A9%E6%99%AE%E5%B8%8C%E8%8C%A8%E9%80%A3%E7%BA%8C  

定义2((L-Bounded支持)，暂无参考
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr2w2qghwqj30vz03hgmm.jpg)  

定义3((1/µ)-平滑)，暂无参考
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr2w7ngsrej30wi04j3zx.jpg)  

定义4((µ-Strong Convexity)，暂无参考
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr2w8badm9j30vy04edh6.jpg)  

#### 2.2 Duality(对偶性)
&emsp;&emsp;已有数种方法被提出用于解决(1)，并且这些方法一般分为两类:原始方法，直接在原始目标上运行；对偶方法，不是在原始目标上运行，而是在对偶形式上运行。在我们开发框架时，允许提出了一个抽象，允许使用原始形式或对偶形式运行。特别是，为了解决输入问题（1），我们考虑将问题映射到以下两个一般问题之一： 

 ![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr3r2lwj7mj30w8093abt.jpg)  
 
&emsp;&emsp;这里α(n维)和w(d维)是参数向量，A=[x1,x2...xn](d*n维)是一个由列向量组成的数据矩阵，其中xi是d维的，f*和g*分别是f和g的凸共轭(??? convex conjugates不会翻译)。  

&emsp;&emsp;问题(A)和(B)的对偶关系被称为Fenchel-Rockafellar对偶。我们在5.1.2节提供了一个自包含求导的对偶。这表明了当对偶问题由典型的一对最大最小问题给出，根据(A)和(B)在框架中的角色，我们等价地重新公式化这两个最小化问题。  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr3ri5hu9bj30wb03jq3j.jpg)  

&emsp;&emsp;这一映射是由对象的F部分的一阶最优性条件产生的，对偶间隙如下：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr3vl2dtmwj30vu02b3yk.jpg)  

&emsp;&emsp;对偶间隙通常是非负的，并且当其值为0时，是强对偶。在出现原始错误或对偶错误时，任意点的对偶间隙提供了一个可计算的上界，因为：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr3vqes5cqj30vx0223yi.jpg)  

&emsp;&emsp;在开发所提出的框架时，注意到(a)和(b)之间的对偶性有许多好处，包括计算对偶间隙的能力并保证了近似值质量。它也是一个有用的分析工具，帮助我们提出一个紧密的框架，并将这项工作与前面的工作[100, 38]和[53, 51]联系起来。作为一个警告，请注意，我们避免将“原始”或“对偶”的名称直接用于问题(A)或(B)中的任一个，正如我们下面所展示的，使用原始或是对偶可以根据应用问题而改变。  

#### 2.3 Assumptions and Problem Cases(假设和问题案例)
&emsp;&emsp;我们在问题(A)上的主要假设是函数f是(1/τ)-平滑，并且函数g是可分离的，例如g(α)=g1(α1)+g2(α2)+...+gn(αn)，每个gi有L-Bounded支持。考虑到问题（a）和（b）之间的对偶，这可以等价地表示为假设在问题(b)中，如下图，本段不译中：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8kopu6h6j30hq02xjs9.jpg)

&emsp;&emsp;为了清楚起见，在表2.1中，我们将关于目标（A）和（B）的假设与一般输入问题（I）联系起来。假设，在方程(I)中，我们想找到一个一般目标的最小化。根据函数l的光滑性和函数r的强凸性，我们将能够根据我们的假设将输入函数（I）映射到目标(A)和(B)中的一个（或两者）。  

&emsp;&emsp;尤其，我们将会描述三种不同的情况：
- 1.函数l是光滑的并且函数r是强凸的
- 2.函数l是光滑的并且函数r是非强凸的并且可分离(写成求和公式那种)
- 3.函数l是分光滑并且可分离(写成求和公式那种)，函数r是强凸的  
&emsp;&emsp;这三种情况将会囊括大多数在线性正则最小化损失问题的应用。在第2.9节中，我们将看到框架的不同变型可以根据这三种情况中的哪一个来解决输入问题（I）。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8l9ir8j1j30h603raag.jpg)  

#### 2.4 Running Examples(运行示例)
&emsp;&emsp;为了阐释表2.1中的三种例子，我们选择了下面的几种例子。这些应用将会作为运行示例贯穿全文，并且我们会在第四章的实验中重新审视它们。更多的应用和细节将会在第三章中提供。
1.弹性网回归(例1，映射A或B)。我们可以映射弹性网正则化最小二乘回归  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8lmt948vj30h001n3ye.jpg)
到目标A或B的任何一个。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8loc981wj30gr02vwfb.jpg)  
我们将在2.9节中讨论对于A和B如何选择弹性网正则的映射将会导致我们框架产生两种不同的变型，并推导出该方法的并行化方案和整体性能。  

2.Lasso(例2，映射到A)。我们可以通过映射模型来表示L1正则化最小二乘回归：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8lz4vtgxj30gq04p74z.jpg)  

3.支持向量机(例3，映射到B)。我们可以通过映射模型来表示关键损失支持向量机(SVM)。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8m20mm2rj30gu04pwfd.jpg)  

#### 2.5 Data Partitioning(数据划分)  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8m8jpoqwj30ho04pgn3.jpg)  
&emsp;&emsp;为了检视我们在分布式环境的设置，我们假定数据集A通过一个划分P分布在超过K台机器上。我们通过nk=|pk|表示机器上划分的大小。对于机器K和权重向量α，我们定义αk作为n元向量的元素，如果i属于Pk并且αki=0。类似地，我们为了A的列的相关组和所有为0的地方写A[k]（注意，列可以对应于训练实例或特征，取决于应用）。我们将在2.9节中讨论这些分布式体系的细节。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8moegbm4j30hw06fdhe.jpg)

#### 2.6 Method(方法)  
&emsp;&emsp;我们这个框架的目的是找到一个A的全局最小解，而分布式计算是基于数据集A的划分。作为第一步，注意到A中项函数g更新分发较为容易，因为我们根据要求划分数据，这一过程是可以分离的，例如g(α)=g1(α1)+g2(α2)+...+gn(αn)。但是这一操作不可以在f(Aα)上进行。为了最小化分布式方法中的目标，我们提出最小化函数的二次近似，这允许在机器间分离最小化过程。在下面的小节中进行了精确的近似。  

&emsp;&emsp;局部数据的二次最优子问题。在一般的CoCoA框架中(算法1)，我们通过定义优化问题A的局部子问题来为每台机器分配计算。这个简单的问题可以在机器k上解决，并且只需访问已经在本地可用的数据，例如列A[k].更具形式地，每台机器k被指定以下的局部子问题，这些子问题仅仅依赖于先前的共享向量v=Aα，并且局部数据A[k]:  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frb4e4n5woj30hl04y759.jpg)  

&emsp;&emsp;这里我们让∆α[k]表示局部变量αi的变化并且我们设定，对于所有的i不属于Pk，有(∆α[k])i=0。值得重点关注的是，子问题(2.10)是简单的，因为它总是一个二次目标(除了gi项)。这一子问题并不依赖于函数f本身，而只依赖于由固定的共享向量的线性化。此属性还简化了局部求解器的任务，特别是对于函数f复杂的情况。  

&emsp;&emsp;框架参数γ和σ。在我们的框架中有两个参数必须被设置：γ，是聚合参数，它控制着如何将每台机器的更新组合；σ，子问题参数，这是一个测量数据划分Pk难度的数据依赖项。这些术语在该方法的收敛中起着至关重要的作用，正如我们在第5章中所演示的。在实践中，我们提供了一个简单而稳健的方法来设置这些参数：对于给定的聚合参数γ属于(0, 1]，子问题参数σ被设置为σ=γK，但是我们也可以按照下面讨论的数据依赖的方式进行改进。大体上，正如我们在第五章所展示的，设定γ=1并且σ=K将会保证收敛同时提供最快的收敛速度。  

&emsp;&emsp;定义5(数据依赖的聚合参数)。在算法1中，聚合参数γ控制了所有机器的部分解的加法与平均值水平。对于我们在第五章的收敛结果，子问题参数σ必须不小于：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frb59qswqbj30hk03wmxb.jpg)  
&emsp;&emsp;在某些情况下，对于σ可能会有更优的(数据依赖)选择，更接近于给定的实际边界σmin。  

&emsp;&emsp;子问题说明。这里，我们提供了局部数据子问题（2.10）背后的进一步直观认识。本地的目标函数G被定义为A中的近似全局目标，Δα[k]是变化的，我们可以在第5章中的分析中看到。事实上，如果子问题被精确地解决，这可以被解释为是一个依赖数据的、块可分的近端目标，应用于A中函数f的部分如下：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frb5jhxxjvj30hl03smxc.jpg)  
&emsp;&emsp;然而，注意到，与传统近端方法相比，我们的算法假设这一子问题会以高精度被解决，因为我们允许使用任何近似质量Θ的局部求解器。  

&emsp;&emsp;现有具有可重用性的单机求解器。我们的局部子问题在结构上与全局问题十分相似，这一属性极具吸引力，二者的主要区别在于它们被定义在较小的(本地)数据子集上，并且更简单，因为它们不依赖于f的结构。对于CoCoA的使用者来说，这是一个主要优点，即现有的单机求解器可以直接对应于我们的分布式框架(算法1),并把它们应用到子问题G上。  

&emsp;&emsp;因此，针对特定问题调整的求解器已经被开发出来，以及相关速度的提升(例如多核实现)，可以很容易地在分布式设置中使用。我们用以下的假设和评论来量化本地求解器性能的依赖性，并将这一性能和第5章的全局收敛率联系起来。  

&emsp;&emsp;假设1(θ近似求解)。我们假定存在一个在[0,1)的θ，而且任意的k属于[K]，求解器在任意外迭代t产生(可能)随机化近似解Δα[k]满足：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frbsym732gj30hk03tjrm.jpg)  

&emsp;&emsp;评论1。在实践中，并行求解局部子问题所花费的事件应选择合适的时机进行通信，从而在给定系统上获得最佳整体效率。我们在第五章中研究了这一平衡的理论并在第四章中实验操作。  

&emsp;&emsp;评论2。注意到，精度参数Θ不必预先选择：如果θ是算法1的回合中实际经验值θ的上界，那么我们的收敛结果就是合理的。这允许K台机器中的某些机器提供时好时坏的精度(例如，如果在在特定的回合提前停止较慢的本地机器，从而避免其他机器等待)。  

&emsp;&emsp;为了使这一框架适当地工作，我们接下来将讨论框架的两种变型——原始形式和对偶形式。在运行框架的原始或对偶时，目标总是以分布式方式求解目标A。二者的主要区别是在输入问题(I)中，这个目标是否被视为输入问题的原始形式还是对偶形式。如果我们将输入(I)目标A，那么目标A将会被视为原始形式。如果将其映射为B，目标A将会被视为对偶形式。我们将这种映射技术精确化，并在下面的章节中讨论它的含义(2.7-2.9)。  

#### 2.7 CoCoA in the Primal(原始形式的CoCoA)
&emsp;&emsp;在框架的分布式原始形式版本中，框架通过将原始问题(I)直接映射到目标A运行并应用算法1中描述的普通CoCoA框架。换言之，我们将问题A视为原始目标，并直接求解问题。  

&emsp;&emsp;从理论角度来看，把A看做原始形式将会允许我们选择非强凸正则化器，因为我们允许gi非强凸。这一设置在早期工作[38, 53, 100]中并没有涉及；以及[51]，并且我们将会在第五章中讨论它的细节，作为一个额外的部分必须被引入到设定原始-对偶率的开发中。  

&emsp;&emsp;在分布式设置中运行框架的原始形式具有重要的实际意义，因为它通常意味着数据时通过特征分布的而不是通过训练点分布的。在这一设置下，外迭代的通信量将会是O(训练点的...)。当特征的数量很多时(在使用稀疏诱导正则化器时常见)，这将有助于减少通信并提高整形性能，正如我们在第四章所展现的那样。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frbuq69of5j30hg03z0td.jpg)  

#### 2.8 CoCoA in the Dual(对偶形式的CoCoA)
&emsp;&emsp;在框架的分布式对偶形式版本中，框架通过将原始问题(I)直接映射到目标B并在对偶A上运行算法1求解问题。换言之，我们将问题B视为原始的，并通过A的对偶求解这个问题。  

&emsp;&emsp;这一版本的框架允许我们选择非平滑损失，例如hinge损失或绝对误差损失，因为gi可以是非平滑的。从实际的角度来看，这个版本的框架通常意味着数据通过训练点分发的，并且对于向量O(#的特征)——将会在每次外迭代中通信中被使用。因此当训练点的数量超过特征数量时，该变型时更可取的。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frbv2h8dbtj30hf04574y.jpg)  

#### 2.9 Primal vs. Dual(原始vs对偶)  
&emsp;&emsp;在表2.2中我们重新回顾了2.3节中的三个例子，它展示了CoCoA中的原始或对偶版本如何被应用到个侦破那个输入问题——l(u)+r(u)中，依赖于函数l和r的特征。尤其是，当设定l是光滑的并且r是强凸，用户也许会选择框架的原始形式(算法2)或对偶形式(算法3)中的任意一个。直观地说，当r失去了强凸性时算法2更可取，而当l瑟吉欧器平滑性时算法3更可取。然而也会有系统相关的方面要考虑。在算法2中，我们通常通过特征来分发数据，在算法3中则通过训练点(这个分布取决于我们定义的得术语n和d，见第三章)。为了降低通信成本，我们可以根据特征的数量或者训练点的数量是否是支配项来选择运行算法2还是算法3。在第4章中，我们通过在真实数据集上比较每一个变型的性能来验证这些想法。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frc0ibwg8uj30hf048aag.jpg)  

&emsp;&emsp;在下面的两个部分，我们提供了更深入了解广义CoCoA框架的形式以及和先前工作的关系。相关工作的拓展讨论将在1.2节中得到。  

#### 2.10 Interpretation(解释)  
&emsp;&emsp;在并行和分布式环境中，有数种已经开发的方法用来求解A和B。我们在1.2节中详细描述，了相关工作，并且这里简要地强调了CoCoA和其他广泛使用的并行化方法之间的主要算法差异。特别地，在分布式计算环境中，我们将CoCoA与小批量和批处理方法对比，例如小批量随机梯度下降或坐标下降、梯度下降进而拟牛顿方法。  

&emsp;&emsp;CoCoA与这些方法相似的地方是，它们都是迭代的，例如，通过函数h更新参数向量α使算法达到最优解：在每个迭代步t：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frc118pp65j30he02ga9z.jpg)  

&emsp;&emsp;直到达到收敛。从坐标的角度来看，更新参数向量α的两种迭代方法包括雅克比方法(Jacobi),其中对α坐标的更新不考虑最近的其他坐标更新，以及Gauss-Seidel，在其中使用最新的信息[11]。特别地，这两个范例在迭代t+1中对坐标i进行更新：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frc18vr0w9j30hf02f0ss.jpg)  

&emsp;&emsp;Jacobi方法更新坐标i不需要不需要来自其他坐标的信息，这使得该方法适合并行化。然而，Gauss-Seidel方法在迭代上收敛的更快，因为它能更快地合并来自其他坐标的信息。在单机求解器中，这种差异是明显的，其中随机方法(得益于新的更新)往往优于它们的批处理部分。  

&emsp;&emsp;典型的小批量方法，例如小批量坐标下降，在每次迭代的坐标子集执行Jacobi式更新。这使得这些方法适合于高水平的并行化。然而，就其串行部分的数据点使用，它们不能如此快地合并信息，因为它们必须等待同步的步骤来更新坐标。随着小批量的规模增长，这能在总体运行时间上降低，甚至能在实践中背离目标[58, 79, 89, 90]。  

&emsp;&emsp;相反，CoCoA尝试结合这两种有吸引力的更新方法。它执行Jacobi式的并行更新，，以对α的坐标块进行并行化，同时允许更快的Gauss-Seidel式在每台机器上更新。并行化方案的这种改变是比简单的小批量或批处理方式改进性能的主要原因之一。  

&emsp;&emsp;CoCoA通过允许在每台机器上执行任意数量的Gauss-Seidel迭代（或任何其他本地解算器）来增加额外的灵活性，这使得框架从低通信量的环境中扩展。在通信之前会进行更多迭代，到更高的通信环境，需要较少的内部迭代。我们将在第4章中看到，这种通信灵活性也极大地提高了整个运行时的运行时间。  

## 2.11 Comparison to ADMM(和ADMM的对比)  

&emsp;&emsp;







