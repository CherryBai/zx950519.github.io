---
layout:     post
title:      CoCoA论文+代码实操
subtitle:   System-Aware Optimization for Machine Learning at Scale
date:       2018-05-06
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - CoCoA
    - Paper
    - Code
---

## 题目——System-Aware Optimization for Machine Learning at Scale
链接：https://escholarship.org/uc/item/20n1k4q8  
作者：Virginia Smith  
时间：August 9, 2017  

## 论文

#### Abstract
&emsp;&emsp;New computing systems have emerged in response to the increasing size and complexity
of modern datasets. For best performance, machine learning methods must be designed to
closely align with the underlying properties of these systems.
In this thesis, we illustrate the impact of system-aware machine learning through the
lens of optimization, a crucial component in formulating and solving most machine learning
problems. Classically, the performance of an optimization method is measured in terms of
accuracy (i.e., does it realize the correct machine learning model? ) and convergence rate
(after how many iterations? ). In modern computing regimes, however, it becomes critical to
additionally consider a number of systems-related aspects for best overall performance. These
aspects can range from low-level details, such as data structures or machine specifications,
to higher-level concepts, such as the tradeoff between communication and computation.
We propose a general optimization framework for machine learning, CoCoA, that gives
careful consideration to systems parameters, often incorporating them directly into the
method and theory. We illustrate the impact of CoCoA in two popular distributed regimes:
the traditional cluster-computing environment, and the increasingly common setting of ondevice
(federated) learning. Our results indicate that by marrying systems-level parameters
and optimization techniques, we can achieve orders-of-magnitude speedups for solving modern
machine learning problems at scale. We corroborate these empirical results by providing
theoretical guarantees that expose systems parameters to give further insight into empirical
performance.  

&emsp;&emsp;由于现代数据集不断增加的大小以及复杂性，新的计算系统应运而生。为了最佳性能，机器学习必须被设计地与系统底层
属性紧密相连。在本文中，我们通过镜像优化(一个解释和解决大多数机器学习问题的重要组件)阐明了系统感知机器学习的影响。具有
典型意义的是，一种优化方法的性能是由其精度衡量(例如，它是否实现了正确的机器学习模型)以及收敛率(经历的多少次迭代)衡量的。
然而在现代计算机体系中，为了整体最佳性能，选择一系列系统相关的方面就变得至关重要。这些方面可以从低级细节，例如数据
结构或机器规格，变化到高级层次，例如通信和计算的平衡。我们提出了一种对机器学习优化一般性优化框架，CoCoA，它仔细考虑了系统
参数，通常将它们直接纳入方法和理论中。我们将在两种流行的分布式系统中阐明CoCoA的影响：传统的集群环境以及越来越流行的ondevice
学习(Google的Federated Learning)。我们的结果表明，通过结合系统级参数和机器进行优化，我们可以实现数量级的加速来解决现代
机器学习的规模问题。我们证实这些经验结果提供理论保证并暴露系统参数，以进一步观察经验性能。

#### Introduction
&emsp;&emsp;面对大规模学习应用的挑战，分布式计算体系结构在现代机器学习中应运而生。分布式体系结构通过提高计算能力和存储容量
来提高可扩展性。保障可扩展性的关键挑战是为分布式机器开发通信和信息协同的高效方法(算法)，并考虑到机器学习算法的特殊需求。  
&emsp;&emsp;在大多数分布式系统中，不同机器间的数据通信较主存中读数据和执行本地计算相比具有更大的开销。此外，计算和通信间的
最佳平衡根据被处理数据变化而变化(往往很大)，使用系统，目标即被优化。因此，分布式方法必须适应灵活的通信计算概况，同时仍然提供
收敛的保证。  
&emsp;&emsp;尽管已经提出了许多分布式优化算法，但小批量最优化方法已经成为了解决这一通信计算交换的最流行例子之一。小批量方法
通过将经典的随机方法推广到在一个时间点处理多个数据点，这有助于通过使每轮通信能够进行更多的分布式计算来缓解通信瓶颈。然而，我
们的需求是减小通信，可以采用增大"小批量方法"的大小，然而这些方法的理论收敛率却随着"小批量"的增大而减小，恢复到了经典(批)梯度
法的速率。实证结果证实了这些理论速率，并且在实践中，小批量方法具有有限的灵活性并适应通信计算折衷，以最大程度地并行执行。此外，
由于小批量方法通常是从特定的单个机器求解器派生的，所以这些方法及其相关的分析通常适合于特定的问题实例，并且当在其受限的问题范围
设置之外应用时，在理论上和实践上也能容忍。  
&emsp;&emsp;在本文中，我提出了一个框架——CoCoA，可以解决这两个基本的限制。首先我们允许任意的本地求解器在每台机器上并行使用。这
允许我们的框架可以直接应用最先进的方法，在分布式设定中使用特定的单机求解器。第二，在我们的框架中，不同机器间的数据共享采用了一种
高度灵活的通信方案。这使得通信量能够容易地适应当前的问题和系统，特别是允许在分布式环境中显著减少通信的情况。  
&emsp;&emsp;在我们的框架中提供这些特征的一个关键步骤是首先为每个机器定义有意义的子问题并行求解，然后以有效的方式组合子问题并更新
解。我们的方法和收敛结果依赖于数据的分布（例如，通过特征或通过训练点），以及我们是否解决了原始或对偶中的问题，某些机器学习目标可以
在分布式设定中更容易地分解成子问题。特别是，我们将常见的机器学习目标归类为几种情况，并使用二元性来帮助分解这些目标。正如我们所展示
的，以这种方式使用原始对偶信息不仅允许高效的方法(例如，与最新的分布式方法相比达到50X倍加速)。但也允许强原始对偶收敛保证和实际利益，
例如计算作为准确性证书和停止标准的对偶间隙。  
#### 1.1 Contributions
&emsp;&emsp;总体框架。我们开发了一个通信有效的原始对偶框架，适用于广泛的一类凸优化问题。值得注意的是，与先前的[38, 51, 100]和[53]
的工作相比，我们的概括的、内聚的框架：  
（1）具体地说，结合L1正则化和其他非强凸正则化的困难情况  
（2）允许通过特征或训练点分发数据的灵活性  
（3）可以在原始或对偶公式上运行，这表明我们有重要的理论和实践意义。  
  
&emsp;&emsp;灵活的沟通和本地解决方案。所提出的框架的两个主要优点是其通信效率和内部采用现成的单机求解器的能力。在现实世界的系统中，
通信与计算的开销可以有很大的不同，因此根据手动设置允许灵活的通信量是有利的。我们的框架正好提供了这样的控制。此外，我们允许在每个机
器上使用任意的求解器，这允许重用现有代码和来自多核或其他优化的好处。  

&emsp;&emsp;原始对偶率。我们为我们的框架推导出收敛速度，利用一种新的方法在分析原始双率非强凸正则化。所提出的技术是相对于简单的平滑
技术（例如，[67, 84）和[110 ]中通过对目标添加小L2项来实施强凸性的显著改进。我们的结果包括原始对偶率和证书的线性正则化损失最小化的
一般类，我们展示了早期的工作如何可以作为一个特殊的情况下导出更一般的方法。  

&emsp;&emsp;实验比较。相比于最先进的大规模机器学习方法，所提出的框架产生数量级的速度提升（高达50×或更大）。我们在真实世界的分布式
数据集展示了这些性能增益并做了广泛的实验。我们还探索框架本身的属性，包括在原始或对偶框架中运行框架的效果。比较算法都是在Apache Spark
中实现的，并且运行在Amazon EC2集群上。我们的代码是开源的，在github.com/gingsmith/proxcocoa中公开可用。  

&emsp;&emsp;联合学习。最后，我们探讨了在各种分布式计算环境中的框架，包括联合学习的新领域，其目的是在低功率设备的大型网络上进行优化。
我们提出了一个CoCoA扩展，Mocha，这是理想的适合处理独特的系统和统计挑战的联合环境。通过对真实世界的联合数据集仿真，我们证明了该方法的
优越的统计性能和经验加速，并提供了一个仔细的理论分析，探讨了系统的挑战，如干扰和容错对保证收敛的影响。

#### 1.2 Related Work
&emsp;&emsp;单机坐标求解器。对于强凸正则化器，现有的经验损失最小化的状态是双坐标（SDCA）〔85〕及其加速变型的随机化坐标上升〔84〕。
与原始随机梯度下降（SGD）方法相比，SDCA族通常是优选的，因为它没有学习速率参数，并且具有更快（几何）收敛保证。有趣的是，类似的趋势
最近在坐标求解器中已经观察到文献使用lasso，但具有原始和双重反转的作用。对于这些问题，原始的坐标下降方法已经成为最新的技术。  

&emsp;&emsp;L1正则化问题的坐标下降可以解释为以光滑部分为目标的二次近似的迭代最小化（如一维牛顿步骤），其次是L1部分产生的收缩步长。
在单坐标更新情况下，这是GLMNET（28106）的核心，并广泛应用于基于L1正则化目标的原始公式的求解器[12, 27, 82，91, 105]。当一次改变一
个以上坐标时，再次采用光滑部分上的二次上界，这导致了Logistic回归的特殊情况下的GLMNET中的双回路方法。这一思想对于分布式设置至关重要
，当主动坐标集与本地机器上的坐标集一致时，这些单机方法非常类似于这里提出的分布式框架。  

&emsp;&emsp;并行方法。对于一般的正则化损失最小化问题，建立了基于随机次梯度下降（SGD）的方法。SGD的几种变型已经被提出用于并行计算，
其中许多基于了异步通信的思想[23, 68]。尽管它们在共享内存系统上具有简单性和竞争性能，但这种方法在分布式环境中的缺点是所需通信量等于
本地读取的数据量，因为每轮每台机器访问一个数据点。这些变型在实践中与在本工作中考虑的更多通信高效的方法不具有竞争性，这允许每个通信
回合能进行更多的本地更新。  

&emsp;&emsp;对于L1正则化目标的具体情况，在[17](shotgun?啥意思，散弹枪？)中提出了并行坐标下降（有和不使用小批次），并在[12]中推广;
它是并行环境中性能最好的求解器之一。当内部求解器是子问题上的单坐标更新时，我们的框架减少成为Shotgun(霰弹枪?)。然而，Shotgun没有被
我们的收敛理论所覆盖，因为它使用了一个潜在不安全的上界β，而不是α，这不能保证满足我们的收敛条件（2.11）。在第4章中，我们用Shotgun
来强调在分布式环境中运行这种高通信(high-communication)方法的不利影响。  

&emsp;&emsp;一次性通信方案。在另一个极端，有一些方法只使用一轮通信[参见57, 62, 109，112, 32 ]。这些方法需要对数据分区做额外的假设，
如果数据按"as is"分布，则通常在实践中不满足，即，如果我们事先没有机会以特定的方式分发数据。此外，如果我们忽略了一台计算机之外的所有
数据，如[86]中所示，有些情况(some)不能保证收敛率超过可以达到的水平。在[8]和[7]中给出了对于给定近似质量所必需通信回合的最小数目的附加
相关下界。  

&emsp;&emsp;小批量方法。小批量方法（使用来自几个训练点或每轮特征的更新）更灵活，并且位于并行和一次性通信方案的两个极端之间。然而，
SGD和坐标下降（CD）（例如，[72, 79, 83，90]）的小批量版本由于小批量的粒度增加而遭受它们的收敛速率下降到批处理梯度下降的速率。这是因为
使用了过期的参数向量W进行了小批量更新，与允许CoCoA的即时本地更新的方法相反。  
&emsp;&emsp;小批量方法的另一个缺点是聚合参数更难调整，因为它可以以最小批量大小出现在任何地方。在实践中计算最佳选择往往是未知的或太具有
挑战性。在CoCoA框架中，不需要调整参数，因为聚合参数和子问题参数可以直接使用第2章（定义5）中所讨论的安全界限来设置。  

&emsp;&emsp;批量求解器。ADMM[16]、梯度下降和拟牛顿方法（如L-BFGS）也经常用于分布式环境中，因为它们的通信需求相对较低。然而，它们需要在
每一轮至少有一个完整的（分布式）批处理梯度计算，因此不允许CoCoA提供的通信和计算之间的平衡。在第4章中，我们包括与ADMM、梯度下降和L-BFGS
变型的实验比较，包括L1设置[3]的正方形受限的有限存储器拟牛顿（OWL QN）。  

&emsp;&emsp;最后，我们注意到，虽然对于COCOA的收敛速度反映了经典的批梯度法在外圆数上的收敛类，但现有的批梯度法却具有较弱的理论，因为它们
不允许局部子问题的一般不精确性。相反，我们的收敛速率直接包含这种近似，而且，对于任意局部求解器来说，比批处理方法开销小得多（在每一轮中，
每一台机器必须精确地通过局部数据完全处理）。这使得CoCoA在分布式环境中更加灵活，因为它可以适应不同的通信成本在实际系统中。我们已经在第4章
中看到，这种灵活性导致在竞争方法上获得显著的性能增益。  

&emsp;&emsp;分布式求解器。利用〔70100, 101, 103〕和〔48〕工作线中的原始对偶结构，CoCoA-v1和CoCoA+框架是第一个允许在分布式环境中使用
每一轮中的弱局部近似质量的任何局部解算器。DISDCA[100]的实际变型，称为DISDCA-P，允许以类似的方式添加到COCOA的更新，但被限制为z坐标下降
（CD）作为局部解算器，并且最初提出没有收敛保证。DISDCA-P、COCO-V1和COCOA+都局限于强凸正则化器，因此不像在这项工作中讨论的CoCoA框架具有
一般性。  

&emsp;&emsp;在L1正则化设置中，与我们的框架相关的方法包括在[56]中的GLMNET的分布式变型。受GLMNET和[105]的启发，[12]和[56]的著作在分布
L1上下文中引入了块对角Hessian上近似的思想。后来的工作[92]专门用于稀疏logistic回归的方法。  

#### 1.3 Organization

&emsp;&emsp;本文的其余部分整理如下。第2章介绍了分布式优化的CoCoA框架。我们首先提供必要的背景，包括标准定义的优化和对偶。使用所提出的原始
对偶结构，然后我们的问题的情况下，我们的框架，我们阐明了一组运行的例子。接下来，我们详细介绍CoCoA，并解释框架如何在分布式环境中以原始或对
偶运行。该方法的两个关键部分是更新方案和子问题的公式化，我们通过探索这些组件的几种解释并将方法与相关工作进行比较来结束本章。  

&emsp;&emsp;所提出的方法适用于机器学习和信号处理中的许多常见问题。第3章详细介绍了可以通过通用COCOA框架实现的几个示例应用程序。对于每一个
应用程序，我们描述了原始对偶设置和算法细节；讨论了我们的框架的收敛性的应用程序，并包括实际问题，如有关本地解算器的信息。  

&emsp;&emsp;第4章和第5章分别提供了我们的经验和理论结果。在第4章中，我们将CoCoA与分布式数据中心设置中的其他最先进的求解器进行了比较。我们的
结果表明，在解决现实世界分布式数据集上常见的机器学习问题的上有着数量级加速。为了补充这些比较，我们探索框架的各种性质，包括原始与对偶式优化的
权衡，通信对框架的影响，以及子问题公式的影响。在第5章中，我们得到了框架的收敛保证，并证明了前面章节中给出的所有其它结果。我们的收敛结果包括
原始对偶率和证书的一般类线性正则化损失最小化。在这些结果中的一个关键贡献是我们能够将局部解算器的性能抽象为每个分布式机器上定义的子问题的近似
解。我们的收敛保证是通过将这个局部收敛与全局解的改进联系起来的。  

&emsp;&emsp;最后，在第6章中，我们将CoCoA扩展到联合学习，这是一个日益常见的场景，其中机器学习模型的训练在分布式设备上进行。这种设置提出了新
的统计挑战，在我们的建模方法、模型的培训，以及新的系统挑战。特别地，诸如非IID数据、散乱器和容错的问题比在典型的数据中心设置中更普遍。为了应
对这些挑战，我们提出Mocha，一种方法（1）利用多任务学习，（2）执行目标的交替最小化，和（3）扩展COCOA以执行联合MTL更新。由此产生的框架具有优
越的统计性能和相对于竞争者的高度灵活的优化方案。我们演示了该方法的实证表现，通过模拟真实世界的联合数据集，并提供仔细的理论分析，探讨联合学习
中普遍存在的挑战对我们的收敛保证的影响。

#### Chapter 2——CoCoA Framework
&emsp;&emsp;在本工作中，我们为形如下面类型的最小化问题开发了一个基本框架： 
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr2v8gijdhj3068027q2r.jpg)  

&emsp;&emsp;对于凸函数l和r。通常第一项是数据上的经验损失，通常取形式：l1(u)+l2(u)+...+li(u)(求和公式展开)，第二项是正则项，例如r(u)=λ||u||。该公式在机器学习和信号处理中十分流行，如支持向量机、线性和Logistics回归、Lasso和稀疏logistic回归以及许多其他方法。  

#### 2.1 Notation
&emsp;&emsp;本论文将使用以下标准定义:  

定义1((L-Lipschitz连续)，参考：https://zh.wikipedia.org/wiki/%E5%88%A9%E6%99%AE%E5%B8%8C%E8%8C%A8%E9%80%A3%E7%BA%8C  

定义2((L-Bounded支持)，暂无参考
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr2w2qghwqj30vz03hgmm.jpg)  

定义3((1/µ)-平滑)，暂无参考
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr2w7ngsrej30wi04j3zx.jpg)  

定义4((µ-Strong Convexity)，暂无参考
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr2w8badm9j30vy04edh6.jpg)  

#### 2.2 Duality(对偶性)
&emsp;&emsp;已有数种方法被提出用于解决(1)，并且这些方法一般分为两类:原始方法，直接在原始目标上运行；对偶方法，不是在原始目标上运行，而是在对偶形式上运行。在我们开发框架时，允许提出了一个抽象，允许使用原始形式或对偶形式运行。特别是，为了解决输入问题（1），我们考虑将问题映射到以下两个一般问题之一： 

 ![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr3r2lwj7mj30w8093abt.jpg)  
 
&emsp;&emsp;这里α(n维)和w(d维)是参数向量，A=[x1,x2...xn](d*n维)是一个由列向量组成的数据矩阵，其中xi是d维的，f*和g*分别是f和g的凸共轭(??? convex conjugates不会翻译)。  

&emsp;&emsp;问题(A)和(B)的对偶关系被称为Fenchel-Rockafellar对偶。我们在5.1.2节提供了一个自包含求导的对偶。这表明了当对偶问题由典型的一对最大最小问题给出，根据(A)和(B)在框架中的角色，我们等价地重新公式化这两个最小化问题。  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr3ri5hu9bj30wb03jq3j.jpg)  

&emsp;&emsp;这一映射是由对象的F部分的一阶最优性条件产生的，对偶间隙如下：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr3vl2dtmwj30vu02b3yk.jpg)  

&emsp;&emsp;对偶间隙通常是非负的，并且当其值为0时，是强对偶。在出现原始错误或对偶错误时，任意点的对偶间隙提供了一个可计算的上界，因为：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr3vqes5cqj30vx0223yi.jpg)  

&emsp;&emsp;在开发所提出的框架时，注意到(a)和(b)之间的对偶性有许多好处，包括计算对偶间隙的能力并保证了近似值质量。它也是一个有用的分析工具，帮助我们提出一个紧密的框架，并将这项工作与前面的工作[100, 38]和[53, 51]联系起来。作为一个警告，请注意，我们避免将“原始”或“对偶”的名称直接用于问题(A)或(B)中的任一个，正如我们下面所展示的，使用原始或是对偶可以根据应用问题而改变。  

#### 2.3 Assumptions and Problem Cases(假设和问题案例)
&emsp;&emsp;我们在问题(A)上的主要假设是函数f是(1/τ)-平滑，并且函数g是可分离的，例如g(α)=g1(α1)+g2(α2)+...+gn(αn)，每个gi有L-Bounded支持。考虑到问题（a）和（b）之间的对偶，这可以等价地表示为假设在问题(b)中，如下图，本段不译中：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8kopu6h6j30hq02xjs9.jpg)

&emsp;&emsp;为了清楚起见，在表2.1中，我们将关于目标（A）和（B）的假设与一般输入问题（I）联系起来。假设，在方程(I)中，我们想找到一个一般目标的最小化。根据函数l的光滑性和函数r的强凸性，我们将能够根据我们的假设将输入函数（I）映射到目标(A)和(B)中的一个（或两者）。  

&emsp;&emsp;尤其，我们将会描述三种不同的情况：
- 1.函数l是光滑的并且函数r是强凸的
- 2.函数l是光滑的并且函数r是非强凸的并且可分离(写成求和公式那种)
- 3.函数l是分光滑并且可分离(写成求和公式那种)，函数r是强凸的  
&emsp;&emsp;这三种情况将会囊括大多数在线性正则最小化损失问题的应用。在第2.9节中，我们将看到框架的不同变型可以根据这三种情况中的哪一个来解决输入问题（I）。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8l9ir8j1j30h603raag.jpg)  

#### 2.4 Running Examples(运行示例)
&emsp;&emsp;为了阐释表2.1中的三种例子，我们选择了下面的几种例子。这些应用将会作为运行示例贯穿全文，并且我们会在第四章的实验中重新审视它们。更多的应用和细节将会在第三章中提供。
1.弹性网回归(例1，映射A或B)。我们可以映射弹性网正则化最小二乘回归  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8lmt948vj30h001n3ye.jpg)
到目标A或B的任何一个。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8loc981wj30gr02vwfb.jpg)  
我们将在2.9节中讨论对于A和B如何选择弹性网正则的映射将会导致我们框架产生两种不同的变型，并推导出该方法的并行化方案和整体性能。  

2.Lasso(例2，映射到A)。我们可以通过映射模型来表示L1正则化最小二乘回归：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8lz4vtgxj30gq04p74z.jpg)  

3.支持向量机(例3，映射到B)。我们可以通过映射模型来表示关键损失支持向量机(SVM)。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8m20mm2rj30gu04pwfd.jpg)  

#### 2.5 Data Partitioning(数据划分)  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8m8jpoqwj30ho04pgn3.jpg)  
&emsp;&emsp;为了检视我们在分布式环境的设置，我们假定数据集A通过一个划分P分布在超过K台机器上。我们通过nk=|pk|表示机器上划分的大小。对于机器K和权重向量α，我们定义αk作为n元向量的元素，如果i属于Pk并且αki=0。类似地，我们为了A的列的相关组和所有为0的地方写A[k]（注意，列可以对应于训练实例或特征，取决于应用）。我们将在2.9节中讨论这些分布式体系的细节。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1fr8moegbm4j30hw06fdhe.jpg)

#### 2.6 Method(方法)  
&emsp;&emsp;我们这个框架的目的是找到一个A的全局最小解，而分布式计算是基于数据集A的划分。作为第一步，注意到A中项函数g更新分发较为容易，因为我们根据要求划分数据，这一过程是可以分离的，例如g(α)=g1(α1)+g2(α2)+...+gn(αn)。但是这一操作不可以在f(Aα)上进行。为了最小化分布式方法中的目标，我们提出最小化函数的二次近似，这允许在机器间分离最小化过程。在下面的小节中进行了精确的近似。  

&emsp;&emsp;局部数据的二次最优子问题。在一般的CoCoA框架中(算法1)，我们通过定义优化问题A的局部子问题来为每台机器分配计算。这个简单的问题可以在机器k上解决，并且只需访问已经在本地可用的数据，例如列A[k].更具形式地，每台机器k被指定以下的局部子问题，这些子问题仅仅依赖于先前的共享向量v=Aα，并且局部数据A[k]:  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frb4e4n5woj30hl04y759.jpg)  

&emsp;&emsp;这里我们让∆α[k]表示局部变量αi的变化并且我们设定，对于所有的i不属于Pk，有(∆α[k])i=0。值得重点关注的是，子问题(2.10)是简单的，因为它总是一个二次目标(除了gi项)。这一子问题并不依赖于函数f本身，而只依赖于由固定的共享向量的线性化。此属性还简化了局部求解器的任务，特别是对于函数f复杂的情况。  

&emsp;&emsp;框架参数γ和σ。在我们的框架中有两个参数必须被设置：γ，是聚合参数，它控制着如何将每台机器的更新组合；σ，子问题参数，这是一个测量数据划分Pk难度的数据依赖项。这些术语在该方法的收敛中起着至关重要的作用，正如我们在第5章中所演示的。在实践中，我们提供了一个简单而稳健的方法来设置这些参数：对于给定的聚合参数γ属于(0, 1]，子问题参数σ被设置为σ=γK，但是我们也可以按照下面讨论的数据依赖的方式进行改进。大体上，正如我们在第五章所展示的，设定γ=1并且σ=K将会保证收敛同时提供最快的收敛速度。  

&emsp;&emsp;定义5(数据依赖的聚合参数)。在算法1中，聚合参数γ控制了所有机器的部分解的加法与平均值水平。对于我们在第五章的收敛结果，子问题参数σ必须不小于：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frb59qswqbj30hk03wmxb.jpg)  
&emsp;&emsp;在某些情况下，对于σ可能会有更优的(数据依赖)选择，更接近于给定的实际边界σmin。  

&emsp;&emsp;子问题说明。这里，我们提供了局部数据子问题（2.10）背后的进一步直观认识。本地的目标函数G被定义为A中的近似全局目标，Δα[k]是变化的，我们可以在第5章中的分析中看到。事实上，如果子问题被精确地解决，这可以被解释为是一个依赖数据的、块可分的近端目标，应用于A中函数f的部分如下：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frb5jhxxjvj30hl03smxc.jpg)  
&emsp;&emsp;然而，注意到，与传统近端方法相比，我们的算法假设这一子问题会以高精度被解决，因为我们允许使用任何近似质量Θ的局部求解器。  

&emsp;&emsp;现有具有可重用性的单机求解器。我们的局部子问题在结构上与全局问题十分相似，这一属性极具吸引力，二者的主要区别在于它们被定义在较小的(本地)数据子集上，并且更简单，因为它们不依赖于f的结构。对于CoCoA的使用者来说，这是一个主要优点，即现有的单机求解器可以直接对应于我们的分布式框架(算法1),并把它们应用到子问题G上。  

&emsp;&emsp;因此，针对特定问题调整的求解器已经被开发出来，以及相关速度的提升(例如多核实现)，可以很容易地在分布式设置中使用。我们用以下的假设和评论来量化本地求解器性能的依赖性，并将这一性能和第5章的全局收敛率联系起来。  

&emsp;&emsp;假设1(θ近似求解)。我们假定存在一个在[0,1)的θ，而且任意的k属于[K]，求解器在任意外迭代t产生(可能)随机化近似解Δα[k]满足：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frbsym732gj30hk03tjrm.jpg)  

&emsp;&emsp;评论1。在实践中，并行求解局部子问题所花费的事件应选择合适的时机进行通信，从而在给定系统上获得最佳整体效率。我们在第五章中研究了这一平衡的理论并在第四章中实验操作。  

&emsp;&emsp;评论2。注意到，精度参数Θ不必预先选择：如果θ是算法1的回合中实际经验值θ的上界，那么我们的收敛结果就是合理的。这允许K台机器中的某些机器提供时好时坏的精度(例如，如果在在特定的回合提前停止较慢的本地机器，从而避免其他机器等待)。  

&emsp;&emsp;为了使这一框架适当地工作，我们接下来将讨论框架的两种变型——原始形式和对偶形式。在运行框架的原始或对偶时，目标总是以分布式方式求解目标A。二者的主要区别是在输入问题(I)中，这个目标是否被视为输入问题的原始形式还是对偶形式。如果我们将输入(I)目标A，那么目标A将会被视为原始形式。如果将其映射为B，目标A将会被视为对偶形式。我们将这种映射技术精确化，并在下面的章节中讨论它的含义(2.7-2.9)。  

#### 2.7 CoCoA in the Primal(原始形式的CoCoA)
&emsp;&emsp;在框架的分布式原始形式版本中，框架通过将原始问题(I)直接映射到目标A运行并应用算法1中描述的普通CoCoA框架。换言之，我们将问题A视为原始目标，并直接求解问题。  

&emsp;&emsp;从理论角度来看，把A看做原始形式将会允许我们选择非强凸正则化器，因为我们允许gi非强凸。这一设置在早期工作[38, 53, 100]中并没有涉及；以及[51]，并且我们将会在第五章中讨论它的细节，作为一个额外的部分必须被引入到设定原始-对偶率的开发中。  

&emsp;&emsp;在分布式设置中运行框架的原始形式具有重要的实际意义，因为它通常意味着数据时通过特征分布的而不是通过训练点分布的。在这一设置下，外迭代的通信量将会是O(训练点的...)。当特征的数量很多时(在使用稀疏诱导正则化器时常见)，这将有助于减少通信并提高整形性能，正如我们在第四章所展现的那样。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frbuq69of5j30hg03z0td.jpg)  

#### 2.8 CoCoA in the Dual(对偶形式的CoCoA)
&emsp;&emsp;在框架的分布式对偶形式版本中，框架通过将原始问题(I)直接映射到目标B并在对偶A上运行算法1求解问题。换言之，我们将问题B视为原始的，并通过A的对偶求解这个问题。  

&emsp;&emsp;这一版本的框架允许我们选择非平滑损失，例如hinge损失或绝对误差损失，因为gi可以是非平滑的。从实际的角度来看，这个版本的框架通常意味着数据通过训练点分发的，并且对于向量O(#的特征)——将会在每次外迭代中通信中被使用。因此当训练点的数量超过特征数量时，该变型时更可取的。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frbv2h8dbtj30hf04574y.jpg)  

#### 2.9 Primal vs. Dual(原始vs对偶)  
&emsp;&emsp;在表2.2中我们重新回顾了2.3节中的三个例子，它展示了CoCoA中的原始或对偶版本如何被应用到个侦破那个输入问题——l(u)+r(u)中，依赖于函数l和r的特征。尤其是，当设定l是光滑的并且r是强凸，用户也许会选择框架的原始形式(算法2)或对偶形式(算法3)中的任意一个。直观地说，当r失去了强凸性时算法2更可取，而当l瑟吉欧器平滑性时算法3更可取。然而也会有系统相关的方面要考虑。在算法2中，我们通常通过特征来分发数据，在算法3中则通过训练点(这个分布取决于我们定义的得术语n和d，见第三章)。为了降低通信成本，我们可以根据特征的数量或者训练点的数量是否是支配项来选择运行算法2还是算法3。在第4章中，我们通过在真实数据集上比较每一个变型的性能来验证这些想法。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frc0ibwg8uj30hf048aag.jpg)  

&emsp;&emsp;在下面的两个部分，我们提供了更深入了解广义CoCoA框架的形式以及和先前工作的关系。相关工作的拓展讨论将在1.2节中得到。  

#### 2.10 Interpretation(解释)  
&emsp;&emsp;在并行和分布式环境中，有数种已经开发的方法用来求解A和B。我们在1.2节中详细描述，了相关工作，并且这里简要地强调了CoCoA和其他广泛使用的并行化方法之间的主要算法差异。特别地，在分布式计算环境中，我们将CoCoA与小批量和批处理方法对比，例如小批量随机梯度下降或坐标下降、梯度下降进而拟牛顿方法。  

&emsp;&emsp;CoCoA与这些方法相似的地方是，它们都是迭代的，例如，通过函数h更新参数向量α使算法达到最优解：在每个迭代步t：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frc118pp65j30he02ga9z.jpg)  

&emsp;&emsp;直到达到收敛。从坐标的角度来看，更新参数向量α的两种迭代方法包括雅克比方法(Jacobi),其中对α坐标的更新不考虑最近的其他坐标更新，以及Gauss-Seidel，在其中使用最新的信息[11]。特别地，这两个范例在迭代t+1中对坐标i进行更新：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frc18vr0w9j30hf02f0ss.jpg)  

&emsp;&emsp;Jacobi方法更新坐标i不需要不需要来自其他坐标的信息，这使得该方法适合并行化。然而，Gauss-Seidel方法在迭代上收敛的更快，因为它能更快地合并来自其他坐标的信息。在单机求解器中，这种差异是明显的，其中随机方法(得益于新的更新)往往优于它们的批处理部分。  

&emsp;&emsp;典型的小批量方法，例如小批量坐标下降，在每次迭代的坐标子集执行Jacobi式更新。这使得这些方法适合于高水平的并行化。然而，就其串行部分的数据点使用，它们不能如此快地合并信息，因为它们必须等待同步的步骤来更新坐标。随着小批量的规模增长，这能在总体运行时间上降低，甚至能在实践中背离目标[58, 79, 89, 90]。  

&emsp;&emsp;相反，CoCoA尝试结合这两种有吸引力的更新方法。它执行Jacobi式的并行更新，，以对α的坐标块进行并行化，同时允许更快的Gauss-Seidel式在每台机器上更新。并行化方案的这种改变是比简单的小批量或批处理方式改进性能的主要原因之一。  

&emsp;&emsp;CoCoA通过允许在每台机器上执行任意数量的Gauss-Seidel迭代（或任何其他本地解算器）来增加额外的灵活性，这使得框架从低通信量的环境中扩展。在通信之前会进行更多迭代，到更高的通信环境，需要较少的内部迭代。我们将在第4章中看到，这种通信灵活性也极大地提高了整个运行时的运行时间。  

## 2.11 Comparison to ADMM(和ADMM的对比)  

&emsp;&emsp;最后，在这一节我们提供CoCoA与ADMM[16]的直观对比。交替方向乘子器（ADMM）是一种公认的分布式优化框架。与CoCoA相似，ADMM不同于前一节中讨论的方法，它定义了并行求解每个问题的子问题，而不是并行化全局批处理或小批量更新。它还利用了二元结构，类似于2.2节中提出的结构。
它还利用了二元结构，类似于第2.2节中提出的结构。  

&emsp;&emsp;对于一致ADMM，将目标B分解为重新参数化：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frc2q2szm0j30hg036jrc.jpg)  

&emsp;&emsp;然后通过构造增广拉格朗日来解决这个问题，它产生如下可分解的更新：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frc2rsp3jvj30hs0adwg3.jpg)  
&emsp;&emsp;对比2.15和2.15我们能看到在特殊例子下，当f(·)=1/2||·||1/2以及我们以(通过算法3)对偶形式求解问题,在每台机器上ADMM和CoCoA选择了一种相思的子问题，但是参数ρ被显式地设置为τ/σ。然而，二者在方法甚至于设置上却有主要的区别。首先，对于全局权重向量W，CoCoA有更简单和直接的更新方案。其次，最重要的是，在CoCoA的方法及理论中，我们允许子问题被近似求解，而不是需要在ADMM中进行全批次更新。我们将在实验中看到这些差异在实践中有很大的影响(第四章)。我们在5.1.3节中提供了与ADMM比较的完整推导。  

## 3. Applications  
&emsp;&emsp;在本章中，我们提供了可以应用在一般CoCoA框架中的示例中的详细处理过程。对于每个例子，我们描述了原始到对偶的建立和算法细节，讨论了我们框架对应用的收敛特性，并包含实际问题，例如最先进的本地求解器信息。我们根据第二章表2.1所定义的三个实例来讨论目标：l(u)+r(u)的极小化，并在表3.1中总结了这些共同的结果。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frfiem1yqyj30mk08gwgc.jpg)  

#### 3.1 Smooth l,Strongly Convex r(光滑的函数l,以及强凸的函数r)  
&emsp;&emsp;对于输入问题1，条件是函数l光滑以及函数r具有强凸性，第五章中的定理6给出了一个全局线性(几何)收敛率。光滑的损失函数可以被映射到目标A中的函数f或是B中的g* 。相似地，强凸正则化可以既可以被映射到目标A中的函数g也可以是B中的f* 。为了说明f作为一个光滑损失函数以及g作为一个目标A中的强凸正则化器的作用，在先前的工作中[38, 51, 53, 100]对比了其典型的作用，我们选择了下面的例子。请注意，映射到目标(B)而不是简单地假设损失在训练点之间是可分离的(见表3.1)。  

&emsp;&emsp;对于本节中的几个例子，我们使用了训练点的数量d的非标准定义和特征数n。有意地使用这些定义，以便我们可以用一个抽象方法(算法1)呈现我们的框架的原始和对偶变化(算法2和3)。  

&emsp;&emsp;Smooth l: least squares loss。使b称为标签或返回值，并选择最小二乘为目标，这是l-smooth的如下：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgeyte3nkj303v00pa9u.jpg)  
在最优化问题(A)中，我们使用下面过程得到了最熟悉的最小二乘回归目标。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgeozbjkoj30hh01rmwz.jpg)  

&emsp;&emsp;观察到f的梯度是△f(v)=v - b，原始对偶映射可以由下面形式给出：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgeu2sqowj306600ldfn.jpg)  
&emsp;&emsp;这就是最小二乘回归中的残差向量。  

&emsp;&emsp;Smooth l: logistic regression loss。对于分类问题，我们选择带有d个训练例子的逻辑回归模型，  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgf2dy6vrj303x00ngle.jpg)  
&emsp;&emsp;yj作为数据矩阵A的行。对于每个训练例子，我们有一个二元标签，这是从向量b中收集的，b只会取正负1,。采用正规形式时，目标被定义为：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgf5gamb2j306p00na9w.jpg)  
&emsp;&emsp;这也是一个可分离的函数，分类损失由下面给出：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgf6iqi13j30hh023a9z.jpg)  
&emsp;&emsp;这里α是参数向量。不难看出，如果标签满足bj只可取正负1，f是l-smooth的。原始对偶映射
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgf98e0zoj306100ojr7.jpg)由下面给出：
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgfabaqm4j304d00wdfn.jpg)  

&emsp;&emsp;Strongly convex r:弹性网络正则(岭回归&Lasso正则化的混合体)。作为一个应用，我们可以选择一个强凸正则器，(A)中的g或(B)中的f*，是弹性网络正则，参数η大于0小于等于1。这可以通过在(A)中设置来获得：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgfwidsk8j30hg03t0ta.jpg)  

#### 3.2 Smooth l, Non-strongly Convex r(光滑的函数l以及非强凸的函数r)

&emsp;&emsp;在例子2中，我们选择将输入问题1映射到目标A，这里假定函数l是光滑的并且函数r非强凸并可分解为求和的形式。对于A中的光滑损失，我们可以选择3.1节中提供的例子，最小二乘损失或logistic损失。对于一个非强凸的正则器，我们在下面选择了一个L1正则的一个重要例子。再一次，我们注意到这一应用不能被目标B实现，这里它被假定正则器f*是强凸的。  
&emsp;&emsp;Non-strongly convex r: L1 regularizer. L1正则化已经在目标A中通过使gi(·)=λ|·|。然而，对于此设定，为了获得原始-对偶收敛以及证明，需要额外的修改。特别地，我们采用在第五章介绍的修改，这将保证L有界。形式上，我们通过下面形式替换gi(·)=λ|·|：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frivke5yl3j30hj01y0sm.jpg)  

&emsp;&emsp;对于足够大的B，这一问题同解于原始的L1目标。注意到这不仅仅影响到理论的收敛性，它允许我们去表现明显的对偶率(Theorem 5 for L=B)。使用这一修改的L1正则器，问题A的优化使用参数λ：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frivrb4hkij30hi01y746.jpg)  

&emsp;&emsp;对于B的值有足够大的选择，这一问题同原始目标同解：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frivsroof4j30hk01r3yf.jpg)  

&emsp;&emsp;修改后的函数g拔只是区间[-3, 3]的绝对值约束版本。因此，通过将B设置成足够大的值，αi的值将永远不会达到，函数g拔将会连续并且同时使3.4等同于3.5。  

&emsp;&emsp;形式上，获得足够大B的值的简单方式使得3.5所有解不受影响，过程如下：如果以α=0我们启动算法，对于执行过程中遇到的每一个解，目标值将永远不会比OA(o)差。形式上，在函数f是非负的假设下，我们将得到(对于每个i)：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1friw4cv99cj30hk01r3yf.jpg)  

&emsp;&emsp;我们因此可以安全地设定B的值为f(0)/λ。对于修改过的gi拔，gi*拔由下面给出：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1friw6vlvvtj30hj037t8s.jpg)  

&emsp;&emsp;Non-strongly convex r: group lasso.批量Lasso惩罚可以被映射到目标A，使用下面形式：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1friw9dnsnhj30hu07r75g.jpg)  

#### 3.3 Non-smooth l, Strongly Convex r(不光滑的函数l，以及强凸函数r)  
&emsp;&emsp;最终，在例子3中，我们选择将输入问题1映射到目标B，这里l被假定为非光滑并且可分离为求和形式，并且r是非强凸的。我们讨论两个常见的函数l的非光滑损失例子，包括用于分类的hinge loss(最大间隔损失函数)和回归的绝对偏差损失。当与强凸正则化器配对时，通过函数f的正则化产生了原始到对偶映射，并且定理5提供了这种形式的目标的次线性收敛率。我们注意到，这种损失不能被直接由目标A实现，这里假定符合函数f的数据时光滑的。  

&emsp;&emsp;Non-smooth l: hinge loss.对于分类问题，我们可以选择hinge loss支持向量机器模型，在n个训练点上，给出的损失形式如下：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1friz4fbh4yj30hs041dgi.jpg)  

&emsp;&emsp;Non-smooth l: absolute deviation loss.绝对误差损失，例如在分位数回归或最小绝对偏差回归中，实现在目标(B)可以通过以下设定：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1friz942shvj30ho036aab.jpg)  
&emsp;&emsp;注：conjugate也许是共轭的意思。  

#### 3.4  Local Solvers(本地求解器)  
&emsp;&emsp;作为第二章中讨论过的，CoCoA框架中，在每台机器上求解子问题，这与全局问题A的结构十分相似，主要的区别在于，它们被定义在较小的(本地)数据子集上，对f来说依赖性更小。因此，已经在单机或多核环境中证明了求解器的价值，并可以很容易地在框架中被使用。我们将在下面讨论一些特殊的本地求解器例子，并向读者指出这些选择的经验性探索(?)。  
&emsp;&emsp;在原始的设定中(算法2),局部子问题(2.10)成为一个本地数据上的简单二次问题，正则化仅适用于局部变量α[k]。对于已经讨论过的L1例子，对于单机算例存在快速的L1求解器，例如glmnet variants[28]或blitz[40]可以直接使用算法1应用到每个本地子问题：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frizqbm8d5j305701h0sl.jpg)  
&emsp;&emsp;由于本地变量α[k]将被级联，每个机器的子问题解的稀疏性自然转化为全局解的稀疏性。  

&emsp;&emsp;就近似质量参数θ而言，对于局部问题(假设1)，我们可以从单机算例上应用已有的最近收敛结果。例如，对于随机坐标下降(作为glmnet的一部分)，Lu andXiao[50, Theorem 1]给出，对于任意一个可分离的正则化器给出一个O(1/t)的近似质量，包括L1和弹性网；可以参看[91]和[92]。  

&emsp;&emsp;在已讨论过的对偶设置(算法3)例子中，损失仅仅被应用于局部(本地)变量α[k],并且正则化器通过二次形式近似(?)。在目前(B)问题中采用随机坐标上升(SDCA)[85]。这一算法及其变型正在实践中被广泛应用[98]，并且其扩展例如加速和并行版本能直接应用于我们的框架。对于非光滑损失例如SVM，[85]的分析提供了O(1/t)的rate(损失率？)，对于光滑损失，一个更快的线性率。最近还有一些对诸如hinge-loss(SVM)的研究可以被应用，例如通过使用错误边界条件[66, 97]，强凸条件弱凸条件或通过选择Polyak-Łojasiewicz条件[41]。  

## 4. Evaluation  
&emsp;&emsp;在本章我们在分布式数据中心的环境下，CoCoA的实验性能。我们首先对两种常见的机器学习应用的计算方法：Lasso回归以及SVM分类进行了比较。然后
我们通过两个变型求解一个弹性网络回归模型，通过对比性能差异来研究CoCoA在原始形式和对偶形式的差别。最终，我们将在4.3.2中说明CoCoA方法的一般性质。  

#### 4.1 Details and Setup(细节以及启动)  
&emsp;&emsp;我们对比了CoCoA和数种最先进的用于大规模优化的方法，包括：  
- MB-SGD：小批量随机下降。对于我们使用Lasso的实验，我们对比了MB-SGD与L1-prox(L1正则)。  
- GD：完全地随机下降。对于Lasso我们使用了近似版本，Prox-GD。  
- L-BFGS：有限内存的拟牛顿方法。对于Lasso，我们使用OWL-QN。
- ADMM：交替方向乘子法。我们在Lasso的实验中使用共轭梯度法，并在SVM实验中使用SDCA。
- MB-CD：小批量并行坐标下降。对于SVM实验，我们实现了MB-SDCA(小批量随机对偶坐标下降)  

&emsp;&emsp;前三种方法已被优化并是现在Apache Spark的机器学习库(v1.5.0)中。我们在大量实验下测试了每种方法的性能表现，下表中展示了数据应用在Lasso, 弹性网络回归，以及SVM模型上。为了对比其他方法，我们将距离映射到最优原始解。通过运行所有方法并多次迭代(很大)，然后选择最小的原始值来计算最优值。所有的代码被写入Apache Spark并且实验运行于Amazon EC2 m3.xlarge的机器上，每个机器只有一个核。我们的代码已发布在：github.com/gingsmith/proxcocoa  

![](http://ww1.sinaimg.cn/large/005L0VzSly1frmhw6mfdvj30j505y0t8.jpg)  

&emsp;&emsp;为了达到最佳性能，我们仔细地在实验中调整计算方法。ADMM需要最多的调整，在选择惩罚参数ρ和解决子问题上。求解ADMM子问题的速度过慢，因此我们需要一个内在的迭代方法并通过允许早停提高性能。我们也允许使用变化的惩罚参数ρ。对于MB-SDCA，我们在每个回合更新，回合的定义是:β/α，其中b是小批量的大小，β的区间为[1, b]，并调整参数b和β。所有方法进一步的推导细节在4.1.1节中。  

&emsp;&emsp;为了简化演示和对比，在下面所有的实验中，我们限制CoCoA仅仅使用简单的最彪下降作为本地求解器。我们注意到甚至CoCoA的强经验性结果可以通过插入当前最先进的本地解算器来获得每个应用程序。  

#### 4.1 Methods for Comparison  

&emsp;&emsp;在本节中，我们通过细节提供了对比的实验装置和方法。所有的实验运行在Amaz EC2集群上的m3.xlarge机器上，每台机器上只有一个核。每种方法的代码由Apache Spark v1.5.0为基础写成。我们的代码是开源的并且发布在：github.com/gingsmith/proxcocoa  

&emsp;&emsp;ADMM：交替方向乘子法[16]是一个流行的方法可以自然地适应分布式环境。对于Lasso回归，对于一个问题要实现ADMM，需要在每台机器上求解一个大的线性系统Cx=d，C是n维的并且n的的范围大于10的7次方，如表4.1中的数据，C是可能的稠密的(非稀疏)。在每台机器上求解是极其缓慢的，因此我们采用共轭梯度法和早停。对于SVM分类，我们使用堆积对偶坐标上升作为一个内部求解器，这一过程在[107]中展示的卓越的性能。我们以后会使用变化的而不是不变的惩罚参数来提高性能例如Boyd做的[16, 3.4.1节]。  

&emsp;&emsp;Mini-batch SGD and proximal GD.小批量SGD是一个在并行及分布式优化中，标准并广泛应用的方法。我们使用Spark的机器学习库中的代码。我们使用网格搜索调整最小批的大小以及SGD的步长。对于Lasso，我们使用方法的近似版本(??? we use the proximal version of the method.)。完全梯度下降可以被视为小批量SGD的特殊情况，这里小批量的大小等同于训练点数量的总和。我们因此也可以使用MLLib库中提供的完全GD，并且使用玩个搜索调整步长参数。  

&emsp;&emsp;Mini-batch CD and SDCA.小批量坐标下降(对于Lasso)和随机对偶坐标上升(对于SVM)目的是通过应用坐标下降提高小批量SGD，其具有理论性和实践性的证明[27, 82, 89, 90, 91]。我们在Spark中实现小批量坐标下降和随机对偶坐标上升并且规定更新模式为每回合，每回合是由β/b定义，b是小批量的大小并且β属于[1, b]，通过网格搜索调整参数b和β。对于Lasso回归的例子，我们实现了Shotgun[17]，这是并行优化的流行方法。Shotgun可以被视为一个特殊的小批量坐标下降，这里小批量可以被看做K，即——每台机器每回合单一更新。我们可以在实验中看到，在分布式环境中频繁通信会变得极其缓慢。









