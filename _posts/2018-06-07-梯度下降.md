---
layout:     post
title:      梯度下降(Gradient Descent)
subtitle:   线性回归中的各种梯度下降
date:       2018-06-07
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 梯度下降
    - 机器学习
    - Python
    - 线性回归
---

## 问题引入

&emsp;&emsp;在机器学习中，我们通常会将求解抽象为一个损失函数，我们的目的是最小化这个损失函数。而梯度下降法和梯度上升法则是一个很好的工具。例如，对于二次函数：  

![](http://ww1.sinaimg.cn/large/005L0VzSly1fs3qd8q7bfj30ob0dhdho.jpg)  

&emsp;&emsp;对于二次函数，我们要求极值，一般我们求导函数，然后找能使导函数为0的点即为所求。对于倒数的精确定义，我们在这里不做精确叙述(参考高数书)。我们可以明确得到的信息如下：  

- 1.在曲线中，导数代表切线的斜率
- 2.导数为负，θ减小，则J增加；θ增加，则J减小
- 3.导数为正，θ减小，则J减小；θ增加，则J增大
- 4.导数可以代表方向，对应了J增大的方向(导数为负，θ减小的方向J增大；导数为正，θ增大的方向J增大)
  
&emsp;&emsp;对于梯度下降法，我们采用步进的方式，一步一步向着极值前进。每次我们都在当前位置，求出下降最快的方向，然后按照设定的步长走一步。在上述例子中，横坐标是θ，我们每次都对横坐标θ进行修改，修改量为框中所示：  

![](http://ww1.sinaimg.cn/large/005L0VzSly1fs3qz4idfgj30o60b7ta0.jpg)  

&emsp;&emsp;其中η代表的就是步长，也叫学习率，是一个大于0的数。如果导数为负，我们在前面添加一个符号，这样得到的新方向就是J减小的方向，这样θ的变化量即为：(-1)*η*(dJ/dθ)。随着不断的前进，我们会达到极值点。值得注意的是，η的取值将会影响获得最优解的速度，某些时候甚至得不到最优解。η是梯度下降法的一个超参数。下面我们来看看η的取值对收敛的影响：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fs3rco4mlfj30qd0dg75z.jpg)  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fs3rd6usejj30qd0dm40h.jpg)  

&emsp;&emsp;此外，对于一些复杂的函数，可能有很多的极值点，因此我们对初始点、学习率的选择可能会使求解结果并非全局最优，即陷入局部最优。  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fs3rfj847pj30qc0dlabg.jpg)  

&emsp;&emsp;解决上述问题的办法就是多次运行，并随机化初始点。  

## 线性回归  

#### 线性回归的目的

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fs3rzenouzj30jr0ezt96.jpg)  

&emsp;&emsp;对于上述给出的一系列散点(二维空间内)，我们的目的是找出一条直线来拟合这些点，反映出这些点大致的潜在规律。  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fs3s3n3rcsj30ka0famxx.jpg)  

- 1.最基本的单变量线性回归：h(x)=theta0+theta1*x1
- 2.复杂点的多变量线性回归：h(x)=theta0+theta1*x1+theta2*x2+theta3*x3

#### 损失函数的形式

&emsp;&emsp;形式如下：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1fs3s9bqpmqj31120rtgm6.jpg)  

&emsp;&emsp;其中θi(0<=i<=n)代表了一个参数，对于n=2的情况，θ0代表了截距，θ1代表斜率。n>2的情况稍微复杂，可以进行类比。损失的定义形式如下：  

![](http://ww1.sinaimg.cn/large/005L0VzSly1fs3smd35ajj3076031wei.jpg)  

&emsp;&emsp;其中m代表点的数量。假设我们已经有了一个模型，对于每个点，我们知道其真实值和由模型预测的值，我们将其做差后平方，这样就得到了该模型在这一个点的损失，然后我们累加所有点的损失就得到了损失函数。从实际意义出发，对于所有点的损失总和，其值越小，说明我们的模型拟合度越高。从几何角度看，我们的模型刻画出的线，使得所有数据点到该线的距离和最小。此外，在线性回归法的损失函数具有唯一的最优解。

## 梯度下降法实现(Python) 

&emsp;&emsp;代码如下：  

```
import numpy as np
import matplotlib.pyplot as plt

def dJ(theta):
    try:
        return 2 * (theta - 2.5)
    except:
        return float('inf')

def J(theta):
    try:
        return (theta - 2.5) ** 2 - 1
    except:
        return float('inf')

def gradient_descent(initial_theta, eta, n_iter = 1e4, epsilon=1e-8):
    theta = initial_theta
    theta_history = [initial_theta]
    i_iter = 0
    while i_iter < n_iter:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
        if (abs(J(theta) - J(last_theta)) < epsilon):
            break

        i_iter += 1

    print("theta值为：" + str(theta))
    print("J(theta)值为：" + str(J(theta)))
    print("theta_histoy长度为：" + str(len(theta_history)))

    return theta_history

def plot_theta_history(plot_x, theta_history):
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
    plt.show()

if __name__ == "__main__":

    # 在-1到6的范围内均匀分割141个点
    plot_x = np.linspace(-1, 6, 141)
    print(plot_x.shape)
    eta = 1.0
    theta_history = gradient_descent(0.0, eta)
    plot_theta_history(plot_x, theta_history)
```
&emsp;&emsp;最终我们打印出相关解：  

```
theta值为：2.499891109642585
J(theta)值为：-0.99999998814289
执行次数为：46
```
&emsp;&emsp;可以看出，结果和实际很接近。

![](http://ww1.sinaimg.cn/large/005L0VzSly1fs3taoxgg3j30gj0daq36.jpg)  

&emsp;&emsp;可以尝试自行修改学习率eta的值，查看不同eta值对收敛的影响。  

## 多元线性回归中的梯度下降法模拟

&emsp;&emsp;在之前的例子中θ是一个值(y=k*x+b中的x)，对于多元情况下，θ代表一个向量(θ=(θ0，θ1，θ2，θ3...，θn))，这样我们就要修改上面的求解形式：  

![](http://ww1.sinaimg.cn/large/005L0VzSly1fs3tkmqeykj30u70fddj9.jpg)  

&emsp;&emsp;在上述公式中，自然地引入了偏导。这里梯度代表了方向，对应J增加最快的方向。考虑下降以及多元的情况，在每个点会有多个下降方向，梯度是所有下降方向中，下降速度最快的方向。下面这个等高线的的图可以清楚地表现出步进的过程：  

![](https://ws1.sinaimg.cn/large/005L0VzSly1fs3ttzw3vkj30eb0eugq3.jpg)  

&emsp;&emsp;当我们使用θ向量(θ=(θ0，θ1，θ2，θ3...，θn))代入后，损失函数变为：  

![](https://ws1.sinaimg.cn/large/005L0VzSly1fs3txvxysdj30tr0ez421.jpg)  

&emsp;&emsp;我们对每个θi求偏导并整理后，得到：  

![](https://ws1.sinaimg.cn/large/005L0VzSly1fs3u9bbggpj30rq0cogpd.jpg)  

&emsp;&emsp;其中X_b代表的是数据矩阵，Xn代表了一个实例(是一个行向量)，这个实例可能有许多特征，每个特征都有其特征值。X_b矩阵等同于(X1,X2,X3,...,Xn)的转置。如下图所示：  

![](https://ws1.sinaimg.cn/large/005L0VzSly1fs3uhuca5qj30ku112tak.jpg)  

&emsp;&emsp;此外，我们要对这个损失函数进行调整，添加系数，最终形式为：  

![](https://ws1.sinaimg.cn/large/005L0VzSly1fs3umr1jbdj30tr0botbz.jpg)  

## 多元梯度下降法实现(Python)

```
import numpy as np
import matplotlib.pyplot as plt

def dJ(theta, X_b, y):
    res = np.empty(len(theta))
    # 单独设定theta0对应的偏导值
    res[0] = np.sum(X_b.dot(theta) - y)
    # 对于theta1-thetan，迭代设置对应的偏导值
    for i in range(1, len(theta)):
        # 对于矩阵来说直接相乘即可，避免了单独求和并累计和，代表的含义参考图中公式即可
        res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
    return res * 2 / len(X_b)

def J(theta, X_b, y):
    try:
        return np.sum((y - (X_b).dot(theta)) ** 2) / len(X_b)
    except:
        return float('inf')

def gradient_descent(X_b, y, initial_theta, eta, n_iter = 1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    while i_iter < n_iter:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        # 更新
        theta = theta - eta * gradient
        # 判断是否收敛
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        i_iter += 1

    print("theta值为：" + str(theta))
    print("J(theta)值为：" + str(J(theta,  X_b, y)))

    return theta

def plot_theta_history(plot_x, theta_history):
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
    plt.show()

if __name__ == "__main__":

    np.random.seed(666)
    x = 2 * np.random.random(size=100)
    y = x * 3. + 4. + np.random.normal(size=100)
    X = x.reshape(-1, 1)

    X_b = np.hstack([np.ones((len(x), 1)), X.reshape(-1, 1)])
    initial_theta = np.zeros(X_b.shape[1])
    eta = 0.01

    theta = gradient_descent(X_b, y, initial_theta, eta)

    print(theta)
    # plt.scatter(x, y)
    # plt.show()
```

&emsp;&emsp;最终我们打印出相关解：  

```
theta值为：[ 4.02145786  3.00706277]
J(theta)值为：1.09887102571
```
&emsp;&emsp;结果与我们的定义的函数：y = x*3.+4.+np.random.normal(size=100)，系数十分接近。

## 多元梯度下降法实现——修改版(Python)







```
def dJ(theta, X_b, y):
    # res = np.empty(len(theta))
    # res[0] = np.sum(X_b.dot(theta) - y)
    # for i in range(1, len(theta)):
    #     res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
    # return res * 2 / len(X_b)
    return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)
```

```
import numpy as np
import matplotlib.pyplot as plt

def dJ(theta, X_b, y):
    # res = np.empty(len(theta))
    # res[0] = np.sum(X_b.dot(theta) - y)
    # for i in range(1, len(theta)):
    #     res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
    # return res * 2 / len(X_b)
    return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)

def dJ_SGD(theta, X_b_i, y_i):
    return X_b_i.T.dot(X_b_i.dot(theta) - y_i) * 2.

def J(theta, X_b, y):
    try:
        return np.sum((y - (X_b).dot(theta)) ** 2) / len(X_b)
    except:
        return float('inf')

def SGD(X_b, y, initial_theta, n_iters):

    t0 = 5
    t1 = 50

    def learning_rate(t):
        return t0 / (t + t1)

    theta = initial_theta
    for cur_iter in range(n_iters):
        rand_i = np.random.randint(len(X_b))
        gradient = dJ_SGD(theta, X_b[rand_i], y[rand_i])
        theta = theta - learning_rate(cur_iter) * gradient

    return theta

def gradient_descent(X_b, y, initial_theta, eta, n_iter = 1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    while i_iter < n_iter:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        i_iter += 1

    print("theta值为：" + str(theta))
    print("J(theta)值为：" + str(J(theta,  X_b, y)))

    return theta

def plot_theta_history(plot_x, theta_history):
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
    plt.show()

if __name__ == "__main__":

    m = 100000
    x = 2 * np.random.random(size=m)
    y = x * 3. + 4. + np.random.normal(0, 3, size=m)
    X = x.reshape(-1, 1)

    X_b = np.hstack([np.ones((len(x), 1)), X])
    # print(X_b)
    initial_theta = np.zeros(X_b.shape[1])
    # print(initial_theta)
    eta = 0.01

    theta = SGD(X_b, y, initial_theta, len(X_b))
    #
    print(theta)
    # plt.scatter(x, y)
    # plt.show()

    theta = gradient_descent(X_b, y, initial_theta, eta)
    print(theta)
```
