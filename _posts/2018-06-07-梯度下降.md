---
layout:     post
title:      梯度下降(Gradient Descent)
subtitle:   线性回归中的各种梯度下降
date:       2018-06-07
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 梯度下降
    - 机器学习
    - Python
    - 线性回归
---

```
import numpy as np
import matplotlib.pyplot as plt

def dJ(theta):
    try:
        return 2 * (theta - 2.5)
    except:
        return float('inf')

def J(theta):
    try:
        return (theta - 2.5) ** 2 - 1
    except:
        return float('inf')

def gradient_descent(initial_theta, eta, n_iter = 1e4, epsilon=1e-8):
    theta = initial_theta
    theta_history = [initial_theta]
    i_iter = 0
    while i_iter < n_iter:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
        if (abs(J(theta) - J(last_theta)) < epsilon):
            break

        i_iter += 1

    print("theta值为：" + str(theta))
    print("J(theta)值为：" + str(J(theta)))
    print("theta_histoy长度为：" + str(len(theta_history)))

    return theta_history

def plot_theta_history(plot_x, theta_history):
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
    plt.show()

if __name__ == "__main__":

    # 在-1到6的范围内均匀分割141个点
    plot_x = np.linspace(-1, 6, 141)
    print(plot_x.shape)
    eta = 1.0
    theta_history = gradient_descent(0.0, eta)
    plot_theta_history(plot_x, theta_history)
```


```
import numpy as np
import matplotlib.pyplot as plt

def dJ(theta, X_b, y):
    res = np.empty(len(theta))
    res[0] = np.sum(X_b.dot(theta) - y)
    for i in range(1, len(theta)):
        res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
    return res * 2 / len(X_b)

def J(theta, X_b, y):
    try:
        return np.sum((y - (X_b).dot(theta)) ** 2) / len(X_b)
    except:
        return float('inf')

def gradient_descent(X_b, y, initial_theta, eta, n_iter = 1e4, epsilon=1e-8):
    theta = initial_theta
    i_iter = 0
    while i_iter < n_iter:
        gradient = dJ(theta, X_b, y)
        last_theta = theta
        theta = theta - eta * gradient
        if (abs(J(theta, X_b, y) - J(last_theta, X_b, y)) < epsilon):
            break
        i_iter += 1

    print("theta值为：" + str(theta))
    print("J(theta)值为：" + str(J(theta,  X_b, y)))

    return theta

def plot_theta_history(plot_x, theta_history):
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
    plt.show()

if __name__ == "__main__":

    np.random.seed(666)
    x = 2 * np.random.random(size=100)
    y = x * 3. + 4. + np.random.normal(size=100)
    X = x.reshape(-1, 1)

    X_b = np.hstack([np.ones((len(x), 1)), X.reshape(-1, 1)])
    initial_theta = np.zeros(X_b.shape[1])
    eta = 0.01

    theta = gradient_descent(X_b, y, initial_theta, eta)

    print(theta)
    # plt.scatter(x, y)
    # plt.show()
```

```
def dJ(theta, X_b, y):
    # res = np.empty(len(theta))
    # res[0] = np.sum(X_b.dot(theta) - y)
    # for i in range(1, len(theta)):
    #     res[i] = (X_b.dot(theta) - y).dot(X_b[:, i])
    # return res * 2 / len(X_b)
    return X_b.T.dot(X_b.dot(theta) - y) * 2. / len(X_b)
```
