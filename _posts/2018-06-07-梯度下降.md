---
layout:     post
title:      梯度下降(Gradient Descent)
subtitle:   线性回归中的各种梯度下降
date:       2018-06-07
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 梯度下降
    - 机器学习
    - Python
    - 线性回归
---

```
import numpy as np
import matplotlib.pyplot as plt

def dJ(theta):
    try:
        return 2 * (theta - 2.5)
    except:
        return float('inf')

def J(theta):
    try:
        return (theta - 2.5) ** 2 - 1
    except:
        return float('inf')

def gradient_descent(initial_theta, eta, n_iter = 1e4, epsilon=1e-8):
    theta = initial_theta
    theta_history = [initial_theta]
    i_iter = 0
    while i_iter < n_iter:
        gradient = dJ(theta)
        last_theta = theta
        theta = theta - eta * gradient
        theta_history.append(theta)
        if (abs(J(theta) - J(last_theta)) < epsilon):
            break

        i_iter += 1

    print("theta值为：" + str(theta))
    print("J(theta)值为：" + str(J(theta)))
    print("theta_histoy长度为：" + str(len(theta_history)))

    return theta_history

def plot_theta_history(plot_x, theta_history):
    plt.plot(plot_x, J(plot_x))
    plt.plot(np.array(theta_history), J(np.array(theta_history)), color='r', marker='+')
    plt.show()

if __name__ == "__main__":

    # 在-1到6的范围内均匀分割141个点
    plot_x = np.linspace(-1, 6, 141)
    print(plot_x.shape)
    eta = 1.0
    theta_history = gradient_descent(0.0, eta)
    plot_theta_history(plot_x, theta_history)
```
