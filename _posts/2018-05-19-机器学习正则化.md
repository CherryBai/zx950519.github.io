---
layout:     post
title:      机器学习正则化参考资料
subtitle:   L0、L1、L2正则化那点事
date:       2018-05-19
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - 正则化
    - L1
    - L2
    - 机器学习
    - 范数规则化
---

## 问题引入

&emsp;&emsp;在机器学习中，通过对训练样本的学习，我们能得到一个看似不错的模型。为什么说看似不错，那是因为该模型往往能在训练样本上具有很高的性能，而在测试样本上性能就很差，就是关键时刻掉链子了！换种说法就是，就好像中国学生一样，擅长应试而不擅长实际应用。对于上面实际存在的问题，我们将其称之为"过拟合"，什么是过拟合，与之对应的还有欠拟合，看下面的例子你就懂了。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgh8nye3hj30t8099gnn.jpg)  
&emsp;&emsp;二维坐标系下，有若干点(x, y)，我们想找到一条曲线(直线),能尽量多地经过这些点。我们选择多项式模型来解决这个问题：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frghgilmqjj31120ku0tu.jpg)  
&emsp;&emsp;如果n太小了，对于数据点的拟合效果将会很差，参考上面组图的左图，几乎没有点落在直线上；当n适中时，对于数据点的拟合效果还可以，参考上面组图的中间图，几乎各点都离曲线很近；而当n很大时，每个点都在曲线上，如上面组图的右图。当n很大时，模型很契合训练数据，而对测试数据来说，往往不能得到上佳的性能。这就是过拟合的表现，拟合的过度了！！！  
&emsp;&emsp;上面是线性回归的情况，逻辑回归与其类似：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frglnvkbxij30pu0cagq3.jpg)  
&emsp;&emsp;为了解决过拟合，我们引用正则化方法。

## 正则化(Regularization)  

#### 含义
&emsp;&emsp;正则化是结构风险最小化策略的实现，是在经验风险上加一个正则项或惩罚项。正则化项一般是复杂度的单调递增函数买模型越复杂，正则化值越大。常见的正则化项是模型参数向量的范数。正则项一般具有如下形式：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgm2ycwffj31120kumyi.jpg)  
&emsp;&emsp;其中λ>=0，代表正则化率。λ过大，模型将会十分简单，但是将会面临欠拟合；λ过小，模型将会十分复杂，将会面临过拟合。理想的λ值不是唯一的，往往根据实际数据的情况而定，需要手动调整。λ后面的一项是范数，将会在后面叙述。

#### 正则项  
&emsp;&emsp;正则项的引入，可以解决模型过拟合的问题。监督机器学习问题旨在正则化参数同时最小化误差。最小化误差要求我们的模型取拟合训练数据，而正则化参数则是方式模型过分拟合训练数据。正则函数的引用可以使我们的模型更加的简单。机器学习中几乎都可以看到在损失函数后面会添加一个额外项，常用的额外项一般有两种，一般英文称作ℓ1-norm和ℓ2-norm，中文称作L1正则化和L2正则化，或者L1范数和L2范数。  

#### 范数  
- L0范数：指向量中非0的元素的个数。
- L1范数：指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”(Lasso regularization)。
- L2范数：指向量中各个元素的平方和然后再求平方根。  

&emsp;&emsp;在绝大多数情况下，我们并不使用L0正则化。因为L0范数很难优化求解(是一个NP难问题)，L1是L0的最优凸近似，比起L0更容易求解。  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgmmyihd7j30tx05bwfx.jpg)  

#### 稀疏  
&emsp;&emsp;为什么要实现稀疏？  

&emsp;&emsp;1.特征选择。一般来说，xi的大部分元素（也就是特征）都是和最终的输出yi没有关系或者不提供任何信息的，在最小化目标函数的时候考虑xi这些额外的特征，虽然可以获得更小的训练误差，但在预测新的样本时，这些没用的信息反而会被考虑，从而干扰了对正确yi的预测。稀疏规则化算子的引入就是为了完成特征自动选择的光荣使命，它会学习地去掉这些没有信息的特征，也就是把这些特征对应的权重置为0。  

&emsp;&emsp;2.可解释性。例如患某种病的概率是y，然后我们收集到的数据x是1000维的，也就是我们需要寻找这1000种因素到底是怎么影响患上这种病的概率的。假设我们这个是个回归模型：y=w1*x1+w2*x2+…+w1000*x1000+b（当然了，为了让y限定在[0,1]的范围，一般还得加个Logistic函数）。通过学习，如果最后学习到的w*就只有很少的非零元素，例如只有5个非零的wi，那么我们就有理由相信，这些对应的特征在患病分析上面提供的信息是巨大的，决策性的。也就是说，患不患这种病只和这5个因素有关，那医生就好分析多了。  

## L1正则化(Lasso)

&emsp;&emsp;假如带有L1正则化的损失函数为：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgn0xu3wej30ai02fdfn.jpg)  
&emsp;&emsp;其中J0是原始的损失函数，加号后面的一项是L1正则化项，α是正则化系数。注意到L1正则化是权值的绝对值之和，J是带有绝对值符号的函数，因此J是不完全可微的。机器学习的任务就是要通过一些方法(比如梯度下降)求出损失函数的最小值。当我们在原始损失函数J0后添加L1正则化项时，相当于对J0做了一个约束(类似拉格朗日函数)。令： 
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgn2w7yfkj304b011dfm.jpg)  
&emsp;&emsp;则J=J0+L，此时我们的任务变成在L约束下求出J0取最小值的解。考虑二维的情况，即只有两个权值w1和w2，此时L=|w1|+|w2|对于梯度下降法，求解J0的过程可以画出等值线，同时L1正则化的函数L也可以在w1, w2的二维平面上画出来。如下图：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgn4g349vj30d90cqjtd.jpg)  
&emsp;&emsp;图中等值线是J0的等值线，黑色方形是L函数的图形。在图中，当J0等值线与L图形首次相交的地方就是最优解。上图中J0与L在L的一个顶点处相交，这个顶点就是最优解。注意到这个顶点的值是(w1, w2)=(0, w)。可以直观想象，因为L函数有很多『突出的角』（二维情况下四个，多维情况下更多），J0与这些角接触的机率会远大于与L其它部位接触的机率(注意，并不是说只有角才可以和J0相交，只是说角与J0相交的概率很大！不过鄙人并不懂为什么在角处特别容易相交)，而在这些角上，会有很多权值等于0。很多权值为0，那么只有少量的权值被保留，说明它们对结果的贡献很大。这就是为什么L1正则化可以产生稀疏模型，进而可以用于特征选择。  

&emsp;&emsp;而正则化前面的系数α，可以控制L图形的大小。α越小，L的图形越大(上图中的黑色方框)；α越大，L的图形就越小，可以小到黑色方框只超出原点范围一点点，这是最优点的值(w1,w2)=(0,w)中的w可以取到很小的值。  

## L2正则化(Ridge)

&emsp;&emsp;假如带有L2正则化的损失函数为：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgncovicbj30er02oq2r.jpg)  
&emsp;&emsp;同理，在二维坐标系下画出图形：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgne6k4dfj30da0c776b.jpg)  
&emsp;&emsp;二维平面下L2正则化的函数图形是个圆，与方形相比，被磨去了棱角。因此J0与L相交时使得w1或w2等于零的机率小了许多，这就是为什么L2正则化不具有稀疏性的原因。L2范数是指向量各元素的平方和然后求平方根。我们让L2范数的正则项||W||2最小，可以使得W的每个元素都很小，都接近于0，但与L1范数不同，它不会让它等于0，而是接近于0。我们在拟合过程中通常都倾向于让各参数权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。可以设想一下对于一个线性回归方程，若参数很大，那么只要数据偏移一点点，就会对结果造成很大的影响；但如果参数足够小，数据偏移得多一点也不会对结果造成什么影响，专业一点的说法是『抗扰动能力强』。  

&emsp;&emsp;为什么L2正则化可以获得值很小的参数？  

&emsp;&emsp;以线性回归中的梯度下降法为例。假设要求的参数为θ，hθ(x)是我们的假设函数，那么线性回归的代价函数如下：  

![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgnsn0hw4j30ak02yt8n.jpg)  

&emsp;&emsp;那么在梯度下降法中，最终用于迭代计算参数θ的迭代式为：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgnth163kj30br02kt8o.jpg)  

&emsp;&emsp;其中α是学习率。上式是没有添加L2正则化项的迭代公式，如果在原始代价函数之后添加L2正则化，则迭代公式会变成下面这样：  
![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgnu9cjhqj30fu02r74a.jpg)  

&emsp;&emsp;其中λ就是正则化参数。从上式可以看到，与未添加L2正则化的迭代公式相比，每一次迭代，θj都要先乘以一个小于1的因子，从而使得θj不断减小，因此总得来看，θ是不断减小的。  


## 对比  

&emsp;&emsp;1. L2正则：使得模型的解偏向于norm较小的W，通过限制W的norm的大小实现了对模型空间的限制，从而在一定程度上避免了过拟合。不过 岭回归并不具有产生稀疏解的能力，得到的系数仍然需要数据中的所有特征才能计算预测结果，从计算量上来说并没有得到改观。  

&emsp;&emsp;2. L1正则：它的优良性质是能产生稀疏性，导致W中许多项变成零。稀疏的解除了计算量上的好处之外，更重要的是更具有“可解释性”。  

&emsp;&emsp;L1和L2正则都是比较常见和常用的正则化项，都可以达到防止过拟合的效果。L1正则化的解具有稀疏性，可用于特征选择。L2正则化的解都比较小，抗扰动能力强。

&emsp;&emsp;Lasso在特征选择时候非常有用，而Ridge就只是一种规则化而已。

![](http://ww1.sinaimg.cn/large/005L0VzSgy1frgo821ddfj31400p0tgg.jpg)







