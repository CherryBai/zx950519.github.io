---
layout:     post
title:      论文选读
subtitle:   论文选读
date:       2018-07-02
author:     Alitria
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Paper
    - 总结
---

## 2018论文选读  

&emsp;&emsp;杭州电子科技大学研究生文献选读任务书  

- 01 System-Aware Optimization for Machine Learning at Scale (老师指定-CoCoA 2017)  
- 02 可扩展机器学习的并行与分布式优化算法综述 (软件学报 2017)  
- 03 An Adaptive Synchronous Parallel Strategy for Distributed Machine Learning (屠杭镝的论文 2017)  
- 04 Parallel Coordinate Descent for L1-Regularized Loss Minimization (01的参考文献，来源未知)  
- 05 基于Fenchel对偶的核Logistic回归并行学习算法 (自动化学报 2011)  
- 06 半监督学习方法 (计算机学报 2014)  
- 07 平行学习—机器学习的一个新型理论框架 (自动化学报 2017)   
- 08 Parallel and Distributed Methods for Constrained Nonconvex Optimization-Part II: Applications in Communications and Machine Learning. (IEEE Trans. Signal Processing 65(8): 1945-1960 2017)  
- 09 A scalable distributed machine learning approach for attack detection in edge computing environments (Journal of Parallel and Distributed Computing 2018)  
- 10 A Parallel Random Forest Algorithm for Big Data in a Spark Cloud Computing Environment (IEEE Transactions on Parallel and Distributed Systems 2017)  
- 11 Communication-efficient Sparse Regression (Journal of Machine Learning Research 2017)  
- 12 Distributed Semi-supervised Learning with Kernel Ridge Regression (Journal of Machine Learning Research 2017)  
- 13 Stochastic Primal-Dual Coordinate Method for Regularized Empirical Risk Minimization (Journal of Machine Learning Research 2017)  
- 14 Distributed Learning with Regularized Least Squares (Journal of Machine Learning Research 2017)
- 15 A distributed block coordinate descent method for training l1 regularized linear classifiers (Journal of Machine Learning Research 2017)  
- 16 Optimal Rates for Multi-pass Stochastic Gradient Methods (Journal of Machine Learning Research 2017)  
- 17 Distributed Bayesian Learning with Stochastic Natural Gradient Expectation Propagation and the Posterior Server (Journal of Machine Learning Research 2017)  
- 18 A General Distributed Dual Coordinate Optimization Framework for Regularized Loss Minimization. (Journal of Machine Learning Research 2017)  
- 19 Second-Order Stochastic Optimization for Machine Learning in Linear Time. (Journal of Machine Learning Research 2017)  
- 20 Gradients Weights improve Regression and Classification (Journal of Machine Learning Research 2016)  
- 21 MLlib: Machine Learning in Apache Spark (Journal of Machine Learning Research 2016)  
- 22 Iterative Hessian Sketch: Fast and Accurate Solution Approximation for Constrained Least-Squares (Journal of Machine Learning Research 2016)  
- 23 Distributed Coordinate Descent Method for Learning with Big Data (Journal of Machine Learning Research 2016)  
- 24 Iterative Regularization for Learning with Convex Loss Functions (Journal of Machine Learning Research 2016)  
- 25 MOCCA: Mirrored Convex/Concave Optimization for Nonconvex Composite Functions (Journal of Machine Learning Research 2016)  
- 26 Optimal Learning Rates for Localized SVMs (Journal of Machine Learning Research 2016)  
- 27 An asynchronous parallel stochastic coordinate descent algorithm (Journal of Machine Learning Research 2015)  
- 28 A framework for machine learning and data mining in the cloud (03的参考文献)  
- 29 Strategies and principles of distributed machine learning on big data (03的参考文献)  
- 30 A distributed framework for scheduled model parallel machine learning (03的参考文献)  
- 31 Distributed machine learning: Foundations, trends, and practices (03的参考文献)  
- 32 Abstract Machine Models for Highly Parallel Computers (03的参考文献)    
- 33 Asynchronous methods for deep reinforcement learning (03的参考文献)  
- 34 More effective distributed ML via a stale synchronous parallel parameter server (03的参考文献)  
- 35 A gradient boosting tree system with parameter server (03的参考文献)  
- 36 A system for large-scale machine learning (03的参考文献)  
- 37 A flexible and efficient machine learning library for heterogeneous distributed systems (03的参考文献)  
- 38 A new platform for distributed machine learning on big data (03的参考文献)  
- 39 Parallelized stochastic gradient descent (03的参考文献)  
- 40 Communication efficient distributed machine learning with the parameter server (03的参考文献)  
- 41 Scaling distributed machine learning with the parameter server (03的参考文献)  
- 42 Scalable hierarchical multitask learning algorithms for conversion optimization in display advertising (Conference
on Web Search and Data Mining 2014)  
- 43 Scalable training of L1-regularized log-linear models (International Conference on Machine Learning. 2007)  
- 44 Communication complexity of distributed convex learning and optimization (Neural Information Processing Systems 2015.)  
- 45 Distributed learning, communication complexity and privacy (Conference on Learning Theory. 2012)  
- 46 Parallel Coordinate Descent Newton Method for Efficient l1-Regularized Minimization (未知来源 2014)  
- 47 Primal-dual rates and certificates (International Conference on Machine Learning. 2016)  
- 48 Accelerated, Parallel, and Proximal Coordinate Descent (SIAM Journal on Optimization 25.4 2015)  
- 49 Communication-efficient distributed dual coordinate ascent (Neural Information Processing Systems. 2014)  
- 50 Collaborating between local and global learning for distributed online multiple tasks (Conference on Information and Knowledge Management. 2015)  
- 51 Solving Large Scale Linear SVM with Distributed Block Minimization (International Conference on Information and Knowledge Management. 2011)  
- 52 Randomized dual coordinate ascent with arbitrary sampling (Neural Information Processing Systems. 2015)  
- 53 Stochastic Dual Newton Ascent for Empirical Risk Minimization (International Conference on Machine Learning. 2016)  
- 54 Accelerated mini-batch stochastic dual coordinate ascent (Neural Information Processing Systems. 2013)  
- 55 Stochastic Dual Coordinate Ascent Methods for Regularized Loss Minimization (Journal of Machine Learning Research 14 2013)  
- 56 Communication-Efficient Distributed Optimization using an Approximate Newton-type Method (International Conference
on Machine Learning. 2014)  
- 57 Distributed Mini-Batch SDCA (未知来源 2015)  
- 58 Mini-Batch Primal and Dual Methods for SVMs (International Conference on Machine Learning. 2013)  
- 59 Large scale distributed sparse precision estimation (Neural Information Processing Systems. 2013)  
- 60 Communication-Efficient Algorithms for Statistical Optimization (Journal of Machine Learning Research 14 2013)  
- 61 Parallel learning-a new framework for machine learning (未知 2017)  
- 62 Scalable training of deep learning machines by incremental block training with intra-block parallel optimization and blockwise model-update filtering (The Journal of Machine Learning Research, 2016)  
- 63 A Survey of Data Mining and Machine Learning Methods for Cyber Security Intrusion Detection (IEEE Communications Surveys & Tutorials 2016)  
- 64 PR-ELM: Parallel regularized extreme learning machine based on cluster (未知 2016)  
- 65 Machine Learning With Big Data:Challenges and Approaches (未知 2017)  
- 66 Angel:a new large-scale machine learning system (National Science Review, 2017)  
- 67 






- 100 
